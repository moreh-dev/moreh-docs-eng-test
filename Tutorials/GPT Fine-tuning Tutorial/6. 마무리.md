---
icon: terminal
tags: [guide]
order: 40
---

# 6. 마무리 

지금까지 MoAI Platform에서 HuggingFace에 공개된 GPT 기반 모델을 fine-tuning하는 과정을 살펴 보았습니다. MoAI Platform을 사용하면 **기존의 학습 코드를 그대로 사용하면서** PyTorch 기반 오픈 소스 LLM 모델을 쉽게 GPU 클러스터에서 fine-tuning할 수 있습니다. 또한, **MoAI 플랫폼을 사용한다면 여러분이 필요한 GPU 수를 코드 변경 없이 손쉽게 설정할 수 있습니다**. 여러분만의 데이터로 새로운 모델을 빠르고 쉽게 개발해 보세요.

# 더 알아보기

- [MoAI Platform의 자동병렬화 기능,  Advanced Parallelization (AP)](https://www.notion.so/MoAI-Platform-Advanced-Parallelization-AP-9f9434db1bc241ee8834b7c7bcaf50d1?pvs=21)
- [Llama2 Fine-tuning](https://www.notion.so/Llama2-Fine-tuning-4a34ac15f9d346f88f73a4add8b6759f?pvs=21)
- [GPT Fine-tuning (KR)](https://www.notion.so/GPT-Fine-tuning-KR-d20d49eca9f54fe7b3f75117ec8f4074?pvs=21)
- [Baichuan2 Fine-tuning](https://www.notion.so/Baichuan2-Fine-tuning-f5cedbed1e704746b864f83db307ffdb?pvs=21)
- [Qwen Fine-tuning](https://www.notion.so/Qwen-Fine-tuning-3f0695d2d1954fefb7e002510b5d7868?pvs=21)