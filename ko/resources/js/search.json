[[{"l":"Korean","p":["Add your Korean documentation within this project.","See also:","English documentation","French documentation"]}],[{"l":"Tutorials","p":["Fine-tuning은 미리 학습된 기계 학습 모델을 새로운 데이터나 특정 작업에 재조정하는 행위를 의미합니다. 즉, AI 모델을 새로운 작업에 적용하고자 할 때, 기존 모델에 새로운 데이터 셋을 학습 시켜 최적화 해나가는 과정인데요. Fine-tuning을 통해 기존 모델을 가져와서 사용자의 필요 및 특정 도메인에 맞게 특화시킬 수 있습니다.","일반적으로 사전학습된 모델을 미세조정(Fine-tuning) 하는데 사전학습된 모델은 범용성을 고려한 매우 큰 파라미터를 가지는 모델이며 큰 모델을 효과적으로 fine-tuning하려면 수백 개에서 수천 개의 예제가 필요합니다.","MoAI Platform에서는 GPU의 메모리 사이즈를 고려해 최적화된 병렬화 기법을 손쉽게 적용할 수 있어 학습 시작 전에 소요되는 시간과 노력을 획기적으로 줄일 수 있습니다.","이 튜토리얼에서는 다음 5종의 모델을 fine-tuning 과정을 배우고, MoAI Platform 을 사용하여 직접 손쉽게 구현하는 방법에 대해 알아보겠습니다."]},{"l":"Fine-tuning Tutorials","p":["Llama2","Mistral","GPT","Qwen","Baichuan2"]},{"l":"Fine-Tuning 방법","p":["모델을 Fine-tuning 과정은 다음과 같습니다:","사전 학습된 모델과 작업에 특화된 데이터셋을 준비합니다.","데이터셋의 예제를 모델에 전달하고 모델의 출력을 수집합니다.","모델의 출력과 예상 출력 간의 손실을 계산합니다.","경사 하강법과 역전파를 사용하여 손실을 줄이기 위해 모델 파라미터를 업데이트합니다.","모델이 수렴될 때까지 여러 epoch에 대해 단계 3-5를 반복합니다.","Fine-tuned 모델은 이제 새로운 데이터에 대한 추론을 위해 배포될 수 있는 상태가 됩니다."]}],[{"l":"Llama2 Fine-tuning","p":["이 튜토리얼은 MoAI Platform에서 오픈 소스 Llama2 13B 모델 을 fine-tuning하는 예시를 소개합니다. 튜토리얼을 통해 MoAI Platform을 통해 AMD GPU 클러스터를 활용하는 방법을 배우고, 성능 및 자동 병렬화의 장점을 확인할 수 있습니다."]},{"l":"개요","p":["Llama2 모델은 2023년 7월에 Meta 가 공개한 Decoder-only Transformer 기반 오픈 소스 모델입니다. 기존 Llama 모델의 구조를 따르지만 40% 더 많은 데이터로 학습시켜 더 다양하고 복잡한 정보를 이해할 수 있습니다.","Llama2는 특히 언어 이해 및 생성 작업에 있어서 뛰어난 성능을 보이며, 다양한 자연어 처리 태스크에서 SOTA 성능을 달성하였습니다. 이 모델은 다국어 지원이 가능하여 전 세계 다양한 언어의 텍스트를 처리할 수 있으며, 공개적으로 접근 가능하여 연구 및 개발 목적으로 널리 사용될 수 있습니다.","이 튜토리얼에서는 MoAI Platform에서 CNN Daily Mail 데이터셋을 활용하여 Llama2 모델을 fine-tuning하는 방법을 살펴보겠습니다. 이 과정에서는 자연어 처리 방법 중 하나인 요약(summarize) 작업을 다룰 것입니다."]},{"l":"시작하기 전에","p":["MoAI Platform 상의 컨테이너 혹은 가상 머신을 인프라 제공자로부터 발급받고, 여기에 SSH로 접속하는 방법을 안내 받으시기 바랍니다. 예를 들어 MoAI Platform 기반으로 운영되는 다음 퍼블릭 클라우드 서비스를 신청하여 사용할 수 있습니다.","KT Cloud의 Hyperscale AI Computing ( https://cloud.kt.com/solution/hyperscaleAiComputing/)","혹은 일시적으로 체험판 컨테이너 및 GPU 자원을 할당 받기를 원하시는 분은 Moreh에 문의하시기 바랍니다.","(Moreh 연락처 정보 추가 예정)","SSH로 접속한 다음 moreh-smi 명령을 실행하여 MoAI Accelerator가 잘 표시되는지 확인하시기 바랍니다. 디바이스 이름은 시스템마다 다르게 설정되어 있을 수 있습니다. 만약 이 과정에 문제가 있다면 인프라 제공자에게 문의하시거나 (troubleshooting 문서 추가 예정) 문서의 가이드를 참고하시기 바랍니다."]},{"l":"MoAI Accelerator 확인","p":["이 튜토리얼에서 안내할 Llama2 모델과 같은 sLLM을 학습하기 위해서는 적절한 크기의 MoAI Accelerator를 선택해야 합니다. 먼저 moreh-smi 명령어를 이용해 현재 사용중인 MoAI Accelerator를 확인합니다.","수행할 학습에 필요한 구체적인 MoAI Accelerator 설정에 대한 설명은 3. 학습 실행하기 에서 제공하겠습니다."]}],[{"l":"1. Fine-tuning 준비하기","p":["MoAI Platform에서 PyTorch 스크립트 실행 환경을 준비하는 것은 일반적인 GPU 서버에서와 크게 다르지 않습니다."]},{"l":"PyTorch 설치 여부 확인하기","p":["SSH로 컨테이너에 접속한 다음 아래와 같이 실행하여 현재 conda 환경에 PyTorch가 설치되어 있는지 확인합니다.","버전명에는 PyTorch 버전과 이를 실행시키기 위한 MoAI 버전이 함께 표시되어 있습니다. 위 예시의 경우 PyTorch 1.13.1+cu116 버전을 실행하는 MoAI의 24.2.0 버전이 설치되어 있음을 의미합니다.","만약 conda: command not found 메시지가 표시되거나, torch 패키지가 리스트되지 않거나, 혹은 torch 패키지가 존재하더라도 버전명에 “moreh”가 포함되지 않은 경우 (Prepare Fine-tuning on MoAI Platform) 문서에 따라 conda 환경을 생성하십시오.","만약 해당 MoAI 버전이 24.2.0이 아닌 다른 버전이라면 아래의 코드를 실행시키십시오."]},{"l":"PyTorch 동작 여부 확인하기","p":["다음과 같이 실행하여 torch 패키지가 정상적으로 import 되고 MoAI Accelerator가 인식되는지 확인합니다. 만약 이 과정에 문제가 생긴다면 (troubleshooting 문서 추가 예정) 문서에 따라 조치하십시오."]},{"l":"필요 Python 패키지 설치","p":["다음과 같이 실행하여 스크립트 실행에 필요한 서드 파티 Python 패키지들을 미리 설치합니다."]},{"l":"학습 스크립트 다운로드","p":["다음과 같이 실행하여 GitHub 레포지토리에서 학습을 위한 PyTorch 스크립트를 다운로드합니다. 본 튜토리얼에서는 tutorial 디렉토리 안에 있는 train_llama2.py 스크립트를 사용할 것입니다."]},{"i":"필요-python-패키지-설치-1","l":"필요 Python 패키지 설치","p":["다음과 같이 실행하여 스크립트 실행에 필요한 서드 파티 Python 패키지들을 미리 설치합니다."]},{"l":"학습 모델 및 토크나이저 다운로드","p":["Hugging Face를 이용해 Llama2-13b-hf 모델의 체크포인트와 토크나이저를 다운로드합니다. 이때 Llama2 모델은 커뮤니티 라이센스 동의와 Hugging Face 토큰 정보가 필요합니다. 또한 Llama2 13B 모델의 경우 체크포인트 용량이 약 49GB이기 때문에 체크포인트를 위한 50GB 스토리지 여유가 필수적입니다.","먼저 다음 사이트에서 필요한 정보를 입력한 후 라이센스 동의를 진행합니다.","meta-llama/Llama-2-13b-hf · Hugging Face","동의서 제출 후 페이지의 상태가 다음과 같이 변경된 것을 확인합니다.","상태 변경이 되었다면, 다음과 같이 tutorial 디렉토리 안의 download_llama2_13b.py 스크립트를 이용해 모델 체크포인트와 토크나이저를 ./llama-2-13b-hf 디렉토리에 다운로드 받을 수 있습니다.","user-token 은 사용자의 Hugging Face 토큰으로 치환합니다.","모델 체크포인트와 토크나이저가 다운로드 받아졌는지 확인합니다."]},{"l":"학습 데이터 다운로드","p":["학습 데이터를 다운로드 받기 위해 dataset 디렉토리 안에 있는 prepare_llama2_dataset.py 스크립트를 사용하겠습니다. 코드를 실행하면 cnn_dailymail 데이터를 다운로드 받고 학습에 사용할 수 있도록 전처리를 진행하여 llama2_dataset.pt 파일로 저장합니다.","저장된 데이터셋은 코드상에서 다음과 같이 로드하여 사용할 수 있습니다."]}],[{"l":"2. Moreh의 학습 코드 톺아보기","p":["학습 데이터를 모두 준비하셨다면 다음으로는 실제 fine-tuning 과정을 실행할 train_llama2.py 스크립트의 내용에 대해 살펴보겠습니다. 이 스크립트는 통상적인 PyTorch 코드로서 Hugging Face Transformers 라이브러리에 있는 Llama2 13B 모델 구현을 기반으로 fine-tuning 작업을 실행합니다.","우선 제공된 스크립트를 그대로 사용하여 튜토리얼을 끝까지 진행해 보시기를 권장합니다. 이후 스크립트를 원하는 대로 수정하셔서 Llama2 13B 모델을 다른 방식으로 fine-tuning 하는 것도 얼마든지 가능합니다. MoAI Platform은 PyTorch와의 완전한 호환성을 제공하기 때문입니다. 필요하시다면 Moreh에서 제공하는 MoAI Platform 응용 가이드( LLM Fine-tuning 파라미터 가이드)를 참고하십시오."]},{"l":"Training Code","p":["모든 코드는 일반적인 pytorch 사용 경험과 완벽하게 동일합니다.","먼저, transformers 라이브러리에서 필요한 모듈을 불러옵니다.","앞서 다운로드 받았던 모델 체크포인트와 토크나이저를 불러옵니다.","1. Fine tuning 준비하기 단계에서 저장한 전처리된 데이터셋을 불러와 데이터로더를 정의합니다.","이후 학습도 일반적인 Pytorch를 사용하여 모델 학습과 동일하게 진행됩니다.","위와 같이 MoAI Platform에서는 기존에 사용하시던 PyTorch 스크립트를 수정 없이 동일하게 사용하실 수 있습니다."]},{"l":"About Advanced Parallelism","p":["본 튜토리얼에 사용되는 학습 스크립트에는 아래와 같은 코드가 추가로 한 줄 존재합니다. 이는 MoAI Platform에서 제공하는 자동 병렬화 기능을 수행하는 코드입니다.","Llama2 13B와 같은 거대 언어 모델은 학습에 많은 양의 GPU가 필요합니다. 따라서 MoAI Platform이 아닌 다른 프레임워크를 사용할 경우, Data Parallelism, Pipeline Parallelism, Tensor Parallelism과 같은 병렬화 기법을 도입하여 학습을 수행해야 합니다.","예를 들어, 사용자가 일반적인 pytorch 코드에서 DDP를 적용하고 싶다면, 다음과 같은 코드 스니펫이 추가되어야 합니다. https://pytorch.org/tutorials/intermediate/ddp_tutorial.html","이와 같은 기본적인 설정 외에도 사용자는 학습 스크립트 작성 과정에서 Python 코드가 다중 처리(multi processing) 환경에서 어떻게 동작하는지 이해해야 하며, 특히 다중 노드(multi node) 설정에서는 학습에 사용되는 각 노드의 환경을 구성해야 합니다. 또한, 모델 종류, 크기, 데이터셋 등을 고려하여 최적의 병렬화 방법을 찾기 위해서는 상당한 시간이 필요합니다.","반면, MoAI Platform의 AP 기능을 통해 사용자는 별도의 병렬화 기법을 적용할 필요 없이, 학습 스크립트에 단 한 줄의 코드를 추가하는 것으로도 최적화된 병렬화 학습을 진행할 수 있습니다.","이렇게 MoAI Platform의 Advanced Parallelization(AP)은 다른 프레임워크에서는 경험하기 어려운 최적화 및 자동화 기능을 제공합니다. AP 기능를 통해 최적의 분산 병렬처리 를 경험해 보시기 바랍니다. AP기능을 이용하면 대규모 모델 훈련 시 필요한 Pipeline Parallelism, Tensor Parallelism의 최적 매개변수와 환경 변수 조합을 아주 간단한 코드 한 줄 로 설정할 수 있습니다."]}],[{"l":"3. 학습 실행하기","p":["이제 실제로 fine tuning을 실행해 보겠습니다."]},{"l":"가속기 Flavor 설정","p":["(모든 문서에 추가될 그림 생성 예정)","AMD MI210 GPU 32개 사용","AMD MI250 GPU 16개 사용","AMD MI300X GPU 8개 사용","KT Cloud의 Hyperscale AI Computing 사용 시: 선택","KT Hyperscale AI Computing (HAC) 서비스 가속기 모델 정보 문서를 참고하십시오.","LLM Fine-tuning 파라미터 가이드","MoAI Platform에서는 사용자에게 물리 GPU가 노출되지 않습니다. 대신에, PyTorch에서 사용할 수 있는 가상의 MoAI Accelerator를 MoAI Accelerator가 제공됩니다. 가속기의 Flavor를 설정함으로써 실제 물리 GPU를 얼마나 활용할지를 결정할 수 있습니다. 선택한 가속기 Flavor에 따라 총 학습 시간과 GPU 사용 비용이 달라지므로 사용자는 학습 상황을 고려하여 결정해야 합니다. 사용자의 학습 목표에 맞는 가속기 Flavor를 선택하기 위해 다음 문서를 참고하세요.","moreh-switch-model 툴을 사용하여 현재 시스템에서 사용 가능한 가속기 flavor 리스트를 확인할 수 있습니다. 원활한 모델 학습을 위해 moreh-switch-model 명령어를 이용해 더 큰 메모리의 MoAI Accelerator로 변경할 수 있습니다.","Moreh의 체험판 컨테이너 사용 시: 선택","q 를 입력해 변경을 완료합니다.","따라서 처음 설정되어 있던 flavor를 로 전환한 다음 moreh-smi 명령을 사용하여 정상적으로 반영되었는지 확인하겠습니다.","로 잘 변경된 것을 확인할 수 있습니다.","먼저 moreh-smi 명령어를 이용해 현재 사용중인 MoAI Accelerator를 확인합니다.","변경 사항이 잘 반영되었는지 확인하기 위해 다시 moreh-smi 명령어를 이용해 현재 사용중인 MoAI Accelerator를 확인합니다.","사용을 위해 8을 입력합니다.","앞서 Llama2 Fine-tuning 문서에서 MoAI Accelerator를 확인했던 것을 기억하시나요? 이제 본격적인 학습 실행을 위해 필요한 가속기를 설정해보겠습니다.","여기서 번호를 입력하여 다른 flavor로 전환할 수 있습니다.","이번 튜토리얼에서는 2048GB 크기의 MoAI Accelerator를 이용하겠습니다.","튜토리얼을 계속 진행하기 위해 인프라 제공자에게 각 flavor에 대응되는 GPU 종류 및 개수를 문의하십시오. 다음 중 하나에 해당하는 Flavor를 선택하여 계속 진행할 수 있습니다.","현재 사용중인 MoAI Accelerator의 메모리 크기는 512GB입니다."]},{"l":"학습 실행","p":["주어진 train_llama2.py 스크립트를 실행합니다.","학습이 정상적으로 진행된다면 다음과 같은 로그가 출력될 것입니다. 이 로그를 통해 Advanced Parallelism 기능이 올바르게 동작하며 최적의 병렬화 설정이 적용되었음을 확인할 수 있습니다. 이전에 살펴본 PyTorch 스크립트 상에서는 AP 코드 한 줄을 제외한 부분에서 GPU 여러 개를 동시에 사용하기 위한 처리가 전혀 없었음을 참고하십시오.","훈련 로그를 확인해 보면 학습이 정상적으로 이루어지는 것을 확인할 수 있습니다.","학습 도중에 출력되는 throughput은 해당 PyTorch 스크립트를 통해 초당 몇 개의 token을 학습하고 있는지를 의미합니다.","AMD MI250 GPU 16개 사용 시: 약 35,000 tokens/sec","GPU 종류 및 개수에 따른 대략적인 학습 소요 시간은 다음과 같습니다.","AMD MI250 GPU 16개 사용 시: 약 10시간"]},{"l":"학습 중에 가속기 상태 확인","p":["학습 도중에 터미널을 하나 더 띄워서 컨테이너에 접속한 후 moreh-smi 명령을 실행하시면 다음과 같이 MoAI Accelerator의 메모리를 점유하며 학습 스크립트가 실행되는 것을 확인하실 수 있습니다. 실행 로그상에서 초기화 과정이 끝나고 Loss가 출력되는 도중에 확인해 보시기 바랍니다.","학습 중에 터미널을 하나 더 열어서 컨테이너에 접속한 후, moreh-smi 명령을 실행하면 MoAI Accelerator가 메모리를 점유하며 학습 스크립트가 실행되는 것을 확인할 수 있습니다. 실행 로그에서 초기화 과정이 끝나고 Loss가 출력되는 중에 확인해 보시기 바랍니다."]}],[{"l":"4. 학습 결과 확인하기","p":["앞 장과 같이 train_llama2.py 스크립트를 실행하면 결과 모델이 llama2_summarization 디렉토리에 저장됩니다. 이는 순수한 PyTorch 모델 파라미터 파일로 MoAI Platform이 아닌 일반 GPU 서버에서도 완벽하게 호환됩니다.","미리 다운로드한 GitHub 레포지토리의 tutorial 디렉토리 아래에 있는 inference_llama2.py 스크립트로 학습된 모델을 테스트해 볼 수 있습니다.","테스트에는 영국 프리미어 리그(EPL) 경기 결과와 관련된 기사 내용이 사용되었습니다.","코드를 실행합니다.","출력값을 확인해보면 Llama2가 프롬프트의 내용을 적절하게 요약한 것을 확인할 수 있습니다."]}],[{"l":"5. GPU 개수 변경하기","p":["앞과 동일한 fine-tuning 작업을 GPU 개수를 바꾸어 다시 실행해 보겠습니다. MoAI Platform은 GPU 자원을 단일 가속기로 추상화하여 제공하며 자동으로 병렬 처리를 수행합니다. 따라서 GPU 개수를 변경하더라도 PyTorch 스크립트를 수정할 필요가 전혀 없습니다."]},{"l":"가속기 Flavor 변경","p":["moreh-switch-model 툴을 사용하여 가속기 flavor를 전환합니다. 가속기 변경 방법은 3. 학습 실행하기 문서를 한번 더 참고해주시기 바랍니다.","인프라 제공자에게 문의하여 다음 중 하나를 선택한 다음 계속 진행하십시오. ( KT Hyperscale AI Computing (HAC) 서비스 가속기 모델 정보)","AMD MI250 GPU 32개 사용","Moreh의 체험판 컨테이너 사용 시: 선택","KT Cloud의 Hyperscale AI Computing 사용 시: 선택","AMD MI210 GPU 64개 사용","AMD MI300X GPU 16개 사용"]},{"l":"학습 실행","p":["다시 train_llama2.py 스크립트를 실행합니다.","사용 가능한 GPU 메모리가 2배 늘었기 때문에, 배치 사이즈 또한 기존 256 에서 512 로 변경하여 실행시켜 보겠습니다.","학습이 정상적으로 진행된다면 다음과 같은 로그가 출력될 것입니다.","앞서 GPU 개수가 절반이었을 때 실행한 결과와 비교해 동일하게 학습이 이루어지며 throughput이 향상되었음을 확인할 수 있습니다.","AMD MI250 GPU 16 → 32개 사용 시: 약 35,000 tokens/sec → 74,000 tokens/sec"]}],[{"l":"6. 마무리","p":["지금까지 MoAI Platform에서 Llama2 13B 모델을 fine-tuning 하는 과정을 살펴보았습니다. Llama와 같은 오픈 소스 LLM은 요약, 질의응답 등 다양한 태스크에 활용할 수 있습니다. MoAI 플랫폼을 사용한다면 여러분이 필요한 GPU 수를 코드 변경 없이 손쉽게 설정할 수 있습니다. 여러분만의 데이터로 새로운 모델을 빠르고 쉽게 개발해 보세요."]},{"l":"더 알아보기","p":["MoAI Platform의 자동병렬화 기능, Advanced Parallelization (AP)","Mistral Fine-tuning","GPT Fine-tuning","Baichuan2 Fine-tuning","Qwen Fine-tuning"]}],[{"l":"Mistral Fine-tuning","p":["이 튜토리얼은 MoAI Platform에서 오픈 소스 Mistral 7B 모델을 fine-tuning하는 예시를 소개합니다. 튜토리얼을 통해 MoAI Platform으로 AMD GPU 클러스터를 사용하는 방법을 익히고 성능 및 자동 병렬화의 이점을 확인할 수 있습니다."]},{"l":"개요","p":["Mistral 모델은 2023년 Mistral AI 사에서 공개한 거대 언어 모델입니다. 코드 생성, 질의 응답, 수학 문제 풀기와 같은 복잡한 태스크에서 더 큰 크기의 모델보다 더 좋은 성능을 기록해 주목 받기도 했습니다.","Mistral 7B 모델은 Transformer의 decoder만을 사용한 Decoder-only 모델입니다. Sliding Window Attention 기법을 적용해 한 번에 처리할 수 있는 입력 토큰의 길이를 크게 늘렸고, Rolling Buffer Cache를 도입해 메모리 사용량을 효율적으로 최적화했습니다.","이 튜토리얼에서는 MoAI Platform에서 코드 생성 태스크에 대해 python_code_instructions_18k-alpaca 데이터셋을 활용하여 Mistral 7B 모델을 fine-tuning 해보겠습니다."]},{"l":"시작하기 전에","p":["MoAI Platform 상의 컨테이너 혹은 가상 머신을 인프라 제공자로부터 발급 받고, 여기에 SSH로 접속하는 방법을 안내 받으시기 바랍니다. 예를 들어 MoAI Platform 기반으로 운영되는 다음 퍼블릭 클라우드 서비스를 신청하여 사용할 수 있습니다.","KT Cloud의 Hyperscale AI Computing ( https://cloud.kt.com/solution/hyperscaleAiComputing/)","혹은 일시적으로 체험판 컨테이너 및 GPU 자원을 할당 받기를 원하시는 분은 Moreh에 문의하시기 바랍니다.","(Moreh 연락처 정보 추가 예정)","SSH로 접속한 다음 moreh-smi 명령을 실행하여 MoAI Accelerator가 잘 표시되는지 확인하시기 바랍니다. 디바이스 이름은 시스템마다 다르게 설정되어 있을 수 있습니다. 만약 이 과정에 문제가 있다면 인프라 제공자에게 문의하시거나 (troubleshooting 문서 추가 예정) 문서의 가이드를 참고하시기 바랍니다."]},{"l":"MoAI Accelerator 확인","p":["이 튜토리얼에서 안내할 Mistral 7B 모델과 같은 sLLM을 학습하기 위해서는 적절한 크기의 MoAI Accelerator를 선택해야 합니다. 먼저 moreh-smi 명령어를 이용해 현재 사용중인 MoAI Accelerator를 확인합니다.","수행할 학습에 필요한 구체적인 MoAI Accelerator 설정에 대한 설명은 “3. 학습 실행하기”에서 제공하겠습니다."]}],[{"l":"1. Fine-tuning 준비하기","p":["MoAI Platform에서 PyTorch 스크립트 실행 환경을 준비하는 것은 일반적인 GPU 서버에서와 크게 다르지 않습니다."]},{"l":"PyTorch 설치 여부 확인하기","p":["SSH로 컨테이너에 접속한 다음 아래와 같이 실행하여 현재 conda 환경에 PyTorch가 설치되어 있는지 확인합니다.","버전명에는 PyTorch 버전과 이를 실행시키기 위한 MoAI 버전이 함께 표시되어 있습니다. 위 예시의 경우 PyTorch 1.13.1+cu116 버전을 실행하는 MoAI의 24.2.0 버전이 설치되어 있음을 의미합니다.","만약 conda: command not found 메시지가 표시되거나, torch 패키지가 리스트되지 않거나, 혹은 torch 패키지가 존재하더라도 버전명에 “moreh”가 포함되지 않은 경우 Prepare Fine-tuning on MoAI Platform 문서에 따라 conda 환경을 생성하십시오."]},{"l":"PyTorch 동작 여부 확인하기","p":["다음과 같이 실행하여 torch 패키지가 정상적으로 import되고 MoAI Accelerator가 인식되는지 확인합니다. 만약 이 과정에 문제가 생긴다면 (troubleshooting 문서 추가 예정) 문서에 따라 조치하십시오."]},{"l":"필요 Python 패키지 설치","p":["다음과 같이 실행하여 스크립트 실행에 필요한 서드 파티 Python 패키지들을 미리 설치합니다."]},{"l":"학습 스크립트 다운로드","p":["다음과 같이 실행하여 GitHub 레포지토리에서 학습을 위한 PyTorch 스크립트를 다운로드합니다. 본 튜토리얼에서는 tutorial 디렉토리 안에 있는 train_mistral.py 스크립트를 사용할 것입니다."]},{"l":"학습 모델 및 토크나이저 다운로드","p":["Hugging Face를 이용해 Mistral 7B v0.1 모델의 체크포인트와 토크나이저를 다운로드 받습니다. 이때 Mistral 모델은 커뮤니티 라이센스 동의와 Hugging Face 토큰 정보가 필요합니다. 또한 Mistral 7B 모델의 경우 체크포인트 용량이 약 15GB이기 때문에 체크포인트를 위한 16GB 이상의 스토리지 여유가 권장됩니다.","먼저 다음 사이트에서 필요한 정보를 입력한 후 라이센스 동의를 진행합니다.","mistralai/Mistral-7B-v0.1 · Hugging Face","동의서 제출 후 페이지의 상태가 다음과 같이 변경된 것을 확인합니다.","상태 변경이 되었다면, 다음과 같이 tutorial 디렉토리 안의 download_mistral_7b.py 스크립트를 이용해 모델 체크포인트와 토크나이저를 ./mistral-7b 디렉토리에 다운로드 받을 수 있습니다.","user-token 은 사용자의 Hugging Face 토큰으로 치환합니다.","모델 체크포인트와 토크나이저가 다운로드 받아졌는지 확인합니다."]},{"l":"학습 데이터 다운로드","p":["이 튜토리얼에서는 코드 생성 훈련을 위해 공개된 여러 데이터셋들 중 Hugging Face에 공개되어 있는 python_code_instructions_18k_alpaca 데이터셋(11.4 MB)을 사용할 것입니다.","prepare_mistral_dataset.py 를 실행해 데이터셋을 다운로드하고, 학습에 사용할 수 있도록 전처리를 진행합니다.","전처리가 진행된 데이터셋은 mistral_dataset.pt 로 저장됩니다.","저장된 데이터셋은 코드상에서 다음과 같이 로드하여 사용할 수 있습니다."]}],[{"l":"2. Moreh의 학습 코드 톺아보기","p":["학습 데이터를 모두 준비하셨다면 다음으로는 실제 fine-tuning 과정을 실행할 train_mistral.py 스크립트의 내용을 살펴 보겠습니다. 이번 단계에서는 MoAI Platform은 pytorch와의 완전한 호환성으로 학습 코드가 일반적인 nvidia gpu를 위한 pytorch 코드와 100% 동일하다는 것을 확인하실 수 있습니다. 또한 이를 넘어서 기존의 복잡한 병렬화 기법들을 MoAI Platform에서는 얼마나 효율적으로 구현할 수 있는지도 확인하실 수 있습니다.","우선 제공된 스크립트를 그대로 사용하여 튜토리얼을 끝까지 진행해 보시기를 권장합니다. 이후 스크립트를 원하는 대로 수정하셔서 Mistral 7B 모델, 혹은 다른 공개된 모델을 다른 방식으로 fine-tuning하는 것도 얼마든지 가능합니다. 필요하시다면 Moreh에서 제공하는 MoAI Platform 응용 가이드( LLM Fine-tuning 파라미터 가이드)를 참고하십시오."]},{"l":"Training Code","p":["모든 코드는 일반적인 pytorch 사용 경험과 완벽하게 동일합니다.","먼저, transformers 라이브러리에서 필요한 모듈을 불러옵니다.","HuggingFace에 공개된 모델 config와 체크포인트를 불러옵니다.","Fine tuning 준비하기 단계에서 저장한 전처리된 데이터셋을 불러와 데이터로더를 정의합니다.","이후 학습도 일반적인 Pytorch를 사용하여 모델 학습과 동일하게 진행됩니다.","위와 같이 MoAI Platform에서는 기존 pytorch 코드와 동일한 방식으로 작성하실 수 있습니다."]},{"l":"About Advanced Parallelism","p":["본 튜토리얼에 사용되는 학습 스크립트에서는 아래와 같은 코드가 추가로 한 줄 존재합니다. 이는 MoAI Platform에서 제공하는 최고의 병렬화 기능을 수행하는 코드입니다.","본 튜토리얼에서 사용하는 Mistral 7B 와 같은 거대한 언어 모델의 경우 필연적으로 여러 개의 GPU를 사용하여 학습시켜야만 합니다. 이때, MoAI Platform이 아닌 다른 프레임워크를 사용할 경우, Data Parallel, Pipeline Parallel , Tensor Parallel과 같은 병렬화 기법을 도입하여 학습을 수행해야 합니다.","예를 들어, 사용자가 일반적인 pytorch 코드에서 DDP를 적용하고 싶다면, 다음과 같은 코드 스니펫이 추가되어야 합니다. ( https://pytorch.org/tutorials/intermediate/ddp_tutorial.html)","이와 같은 기본적인 세팅 이외에도 유저는 학습 스크립트 작성 과정에서 multi processing 환경에서의 Python 코드의 동작에 대해 이해하고 있어야 하며, 특히 multi node 세팅에서는 학습에 사용되는 노드들에 대한 환경 구성 작업이 추가적으로 들어가야 합니다. 게다가 모델의 종류, 크기, 데이터셋 등을 고려한 최적의 병렬화 방법을 찾기 위해서는 매우 많은 시간이 소요됩니다.","반면, MoAI Platform의 AP기능은 유저가 직접 이러한 추가적인 병렬화 기법을 적용할 필요 없이, 단지 학습 스크립트에 다음과 같은 코드 한 줄을 추가하는 것 만으로도 최적화된 병렬화 학습을 진행할 수 있습니다.","이렇듯 다른 프레임워크에서는 경험할 수 없는 병렬화의 최적화 및 자동화 기능인 MoAI Platform만의 Advanced Parallelization(AP)을 통해 최적의 분산 병렬처리 를 경험해보시기 바랍니다. AP기능을 이용하면 일반적으로 대규모 모델 훈련시 필요한 Pipeline Parallelism, Tensor Parallelism의 최적 매개변수와 환경변수 조합을 아주 간단한 코드 한 줄 을 통해 확보할 수 있습니다."]}],[{"l":"3. 학습 실행하기","p":["이제 실제로 fine tuning을 실행해 보겠습니다."]},{"l":"가속기 Flavor 설정","p":["(모든 문서에 추가될 그림 생성 예정)","AMD MI210 GPU 32개 사용","AMD MI250 GPU 16개 사용","AMD MI300X GPU 8개 사용","KT Cloud의 Hyperscale AI Computing 사용 시: 선택","KT Hyperscale AI Computing (HAC) 서비스 가속기 모델 정보 문서를 참고하십시오.","LLM Fine-tuning 파라미터 가이드","MoAI Platform에서는 사용자에게 물리 GPU가 노출되지 않습니다. 대신 PyTorch에서 사용 가능한 가상의 MoAI Accelerator가 제공됩니다. 가속기의 flavor를 설정함으로써 실제로 PyTorch에서 물리 GPU를 얼마나 활용할지를 결정할 수 있습니다. 선택한 가속기 Flavor에 따라 학습 총 시간 및 gpu 사용 비용이 달라지므로 사용자의 학습 상황에 따른 판단이 필요합니다. 사용자의 학습 목표에 맞는 가속기 Flavor를 선택하기 위해 다음 문서를 참고하세요.","moreh-switch-model 툴을 사용하여 현재 시스템에서 사용 가능한 가속기 flavor 리스트를 확인할 수 있습니다. 원활한 모델 학습을 위해 moreh-switch-model 명령어를 이용해 더 큰 메모리의 MoAI Accelerator로 변경할 수 있습니다.","Moreh의 체험판 컨테이너 사용 시: 선택","q 를 입력해 변경을 완료합니다.","따라서 처음 설정되어 있던 flavor를 로 전환한 다음 moreh-smi 명령을 사용하여 정상적으로 반영되었는지 확인하겠습니다.","로 잘 변경된 것을 확인할 수 있습니다.","먼저 moreh-smi 명령어를 이용해 현재 사용중인 MoAI Accelerator를 확인합니다.","변경 사항이 잘 반영되었는지 확인하기 위해 다시 moreh-smi 명령어를 이용해 현재 사용중인 MoAI Accelerator를 확인합니다.","사용을 위해 8을 입력합니다.","앞서 Mistral Fine-tuning 문서에서 MoAI Accelerator를 확인했던 것을 기억하시나요? 이제 본격적인 학습 실행을 위해 필요한 가속기를 설정해보겠습니다.","여기서 번호를 입력하여 다른 flavor로 전환할 수 있습니다.","이번 튜토리얼에서는 2048GB 크기의 MoAI Accelerator를 이용하겠습니다.","튜토리얼을 계속 진행하기 위해 인프라 제공자에게 각 flavor에 대응되는 GPU 종류 및 개수를 문의하십시오. 다음 중 하나에 해당하는 flavor를 선택하여 계속 진행하십시오.","현재 사용중인 MoAI Accelerator의 메모리 크기는 64GB입니다."]},{"l":"학습 실행","p":["주어진 train_mistral.py 스크립트를 실행합니다.","학습이 정상적으로 진행된다면 다음과 같은 로그가 출력 될 것입니다. 중간에 파란색으로 표시된 부분을 보시면 Advanced Parallelism 기능이 정상 동작하는 것을 확인할 수 있습니다. 앞서 살펴 본 PyTorch 스크립트 상에서는 GPU 여러 개를 동시에 사용하기 위한 처리가 전혀 없었음을 참고하십시오.","Loss 값이 다음과 같이 나타나며 정상 학습이 이루어지는 것을 확인할 수 있습니다.","학습 도중에 출력되는 throughput은 해당 PyTorch 스크립트를 통해 초당 몇 개의 token을 학습하고 있는지를 의미합니다.","AMD MI250 GPU 16개 사용 시: 약 60,000 tokens/sec","GPU 종류 및 개수에 따른 대략적인 학습 소요 시간은 다음과 같습니다.","AMD MI250 GPU 16개 사용 시: 약 50분"]},{"l":"학습 중에 가속기 상태 확인","p":["학습 중에 터미널을 하나 더 열어서 컨테이너에 접속한 후에 moreh-smi 명령을 실행하면 MoAI Accelerator의 메모리를 차지하며 학습 스크립트가 실행되고 있는 것을 확인할 수 있습니다. 실행 로그를 보면 초기화가 완료되고 Loss가 출력되는 도중에 확인해 보시기 바랍니다."]}],[{"l":"4. 학습 결과 확인하기","p":["앞 장과 같이 train_mistral.py 스크립트를 실행하면 결과 모델이 mistral_code_generation 디렉터리에 저장됩니다. 이는 순수한 PyTorch 모델 파라미터 파일로 MoAI Platform이 아닌 일반 GPU 서버에서도 100% 호환됩니다.","미리 다운로드한 GitHub 레포지토리의 tutorial 디렉토리 아래에 있는 inference_mistral.py 스크립트로 학습된 모델을 테스트해 볼 수 있습니다. 테스트에는 ‘주어진 문자열 리스트를 입력 받아 공백으로 결합하는 함수를 만들어’라는 프롬프트가 사용되었습니다.","코드를 실행합니다.","출력값을 확인해보면 모델이 프롬프트 내용대로 적절한 함수를 생성한 것을 확인할 수 있습니다."]}],[{"l":"5. GPU 개수 변경하기","p":["앞과 동일한 fine-tuning 작업을 GPU 개수를 바꾸어 다시 실행해 보겠습니다. GPU 개수를 변경하여 다시 실행해 보겠습니다. MoAI Platform은 GPU 자원을 단일 가속기로 추상화하여 제공하므로 자동으로 병렬 처리를 수행합니다. 따라서 GPU 개수를 변경하더라도 PyTorch 스크립트를 전혀 수정할 필요가 없습니다."]},{"l":"가속기 Flavor 변경","p":["moreh-switch-model 을 사용하여 가속기 flavor를 전환합니다. 가속기 변경 방법은 3. 학습 실행하기 문서를 한번 더 참고해주시기 바랍니다.","인프라 제공자에게 문의하여 다음 중 하나를 선택한 다음 계속 진행하십시오. ( KT Hyperscale AI Computing (HAC) 서비스 가속기 모델 정보)","AMD MI250 GPU 32개 사용","Moreh의 체험판 컨테이너 사용 시: 선택","KT Cloud의 Hyperscale AI Computing 사용 시: 선택","AMD MI210 GPU 64개 사용","AMD MI300X GPU 16개 사용"]},{"l":"학습 실행","p":["배치 사이즈 변경 없이 train_mistral.py 스크립트를 실행합니다.","학습이 정상적으로 진행된다면 다음과 같은 로그가 출력될 것입니다.","앞서 GPU 개수가 절반이었을 때 실행한 결과와 비교해 동일하게 학습이 이루어지며 throughput이 향상되었음을 확인할 수 있습니다.","AMD MI250 GPU 16 → 32개 사용 시: 약 60,000 tokens/sec → 110,000 tokens/sec"]}],[{"l":"6. 마무리","p":["지금까지 MoAI Platform에서 Mistral 7B 모델을 fine-tuning하는 과정을 살펴 보았습니다. MoAI Platform을 사용하면 기존의 학습 코드를 그대로 사용하면서 PyTorch 기반 오픈 소스 LLM 모델을 쉽게 GPU 클러스터에서 fine-tuning할 수 있습니다. 또한, MoAI 플랫폼을 사용한다면 여러분이 필요한 GPU 수를 코드 변경 없이 손쉽게 설정할 수 있습니다. 여러분만의 데이터로 새로운 모델을 빠르고 쉽게 개발해 보세요."]},{"l":"더 알아보기","p":["MoAI Platform의 자동병렬화 기능, Advanced Parallelization (AP)","Llama2 Fine-tuning","GPT Fine-tuning","Baichuan2 Fine-tuning","Qwen Fine-tuning"]}],[{"l":"GPT Fine-tuning","p":["이 튜토리얼은 MoAI Platform에서 Hugging Face 에 오픈소스로 공개된 GPT 기반의 모델을 fine-tuning하는 예시를 소개합니다. 튜토리얼을 통해 MoAI Platform으로 AMD GPU 클러스터를 사용하는 방법을 익히고 성능 및 자동 병렬화의 이점을 확인할 수 있습니다."]},{"l":"개요","p":["GPT는 Transformer decoder 구조만을 사용한 언어 모델 아키텍처로써, 2018년 OpenAI 에서 GPT-1을 통해 처음 공개하였습니다. 이후 OpenAI에서는 사전학습에 사용되는 데이터셋 크기와 모델 파라미터를 늘려가며 GPT-2를 거쳐 GPT-3, GPT-4 모델을 개발했으며 이 중 오픈소스로 공개된 모델은 GPT-1, GPT-2 입니다.","GPT의 기본적인 아키텍처는 오픈소스로 공개되어 있기 때문에 Huggingface에서는 OpenAI에서 개발한 모델 이외에도 다양한 GPT 기반의 모델을 찾아볼 수 있습니다.","이 튜토리얼에서는 코드 생성 태스크에 대해 MoAI Platform을 활용해 Cerebras-GPT-13B 모델을 fine-tuning 해보겠습니다."]},{"l":"시작하기 전에","p":["MoAI Platform 상의 컨테이너 혹은 가상 머신을 인프라 제공자로부터 발급받고, 여기에 SSH로 접속하는 방법을 안내 받으시기 바랍니다. 예를 들어 MoAI Platform 기반으로 운영되는 다음 퍼블릭 클라우드 서비스를 신청하여 사용할 수 있습니다.","KT Cloud의 Hyperscale AI Computing ( https://cloud.kt.com/solution/hyperscaleAiComputing/)","혹은 일시적으로 체험판 컨테이너 및 GPU 자원을 할당 받기를 원하시는 분은 Moreh에 문의하시기 바랍니다.","(Moreh 연락처 정보 추가 예정)","SSH로 접속한 다음 moreh-smi 명령을 실행하여 MoAI Accelerator가 잘 표시되는지 확인하시기 바랍니다. 디바이스 이름은 시스템마다 다르게 설정되어 있을 수 있습니다. 만약 이 과정에 문제가 있다면 인프라 제공자에게 문의하시거나 (troubleshooting 문서 추가 예정) 문서의 가이드를 참고하시기 바랍니다."]},{"l":"MoAI Accelerator 확인","p":["이 튜토리얼에서 안내할 GPT 모델과 같은 sLLM을 학습하기 위해서는 적절한 크기의 MoAI Accelerator를 선택해야 합니다. 먼저 moreh-smi 명령어를 이용해 현재 사용중인 MoAI Accelerator를 확인합니다.","수행할 학습에 필요한 구체적인 MoAI Accelerator 설정에 대한 설명은 3. 학습 실행하기 에서 제공하겠습니다."]}],[{"l":"1. Fine-tuning 준비하기","p":["MoAI Platform에서 PyTorch 스크립트 실행 환경을 준비하는 것은 일반적인 GPU 서버에서와 크게 다르지 않습니다."]},{"l":"PyTorch 설치 여부 확인하기","p":["SSH로 컨테이너에 접속한 다음 아래와 같이 실행하여 현재 conda 환경에 PyTorch가 설치되어 있는지 확인합니다.","버전명에는 PyTorch 버전과 이를 실행시키기 위한 MoAI 버전이 함께 표시되어 있습니다. 위 예시의 경우 PyTorch 1.13.1+cu116 버전을 실행하는 MoAI의 24.2.0 버전이 설치되어 있음을 의미합니다.","만약 conda: command not found 메시지가 표시되거나, torch 패키지가 리스트되지 않거나, 혹은 torch 패키지가 존재하더라도 버전명에 “moreh”가 포함되지 않은 경우 (Prepare Fine-tuning on MoAI Platform) 문서에 따라 conda 환경을 생성하십시오."]},{"l":"PyTorch 동작 여부 확인하기","p":["다음과 같이 실행하여 torch 패키지가 정상적으로 import되고 MoAI Accelerator가 인식되는지 확인합니다. 만약 이 과정에 문제가 생긴다면 (troubleshooting 문서 추가 예정) 문서에 따라 조치하십시오."]},{"l":"필요 Python 패키지 설치","p":["다음과 같이 실행하여 스크립트 실행에 필요한 서드 파티 Python 패키지들을 미리 설치합니다."]},{"l":"학습 스크립트 다운로드","p":["다음과 같이 실행하여 GitHub 레포지토리에서 학습을 위한 PyTorch 스크립트를 다운로드합니다. 본 튜토리얼에서는 tutorial 디렉토리 안에 있는 train_gpt.py 스크립트를 사용할 것입니다."]},{"l":"학습 데이터 다운로드","p":["Huggingface에서는 model checkpoint뿐만 아니라 model fine-tuning에 사용할 수 있는 다양한 데이터셋이 공개되어 있습니다.","이번 튜토리얼에서 저희는 mlabonne/Evol-Instruct-Python-26k 데이터셋을 사용할 것입니다. 이 데이터셋은 주어진 질문 조건과 주어진 질문 조건에 맞게 작성된 python 코드로 이루어져 있습니다.","학습 데이터를 다운로드 받기 위해 dataset 디렉토리 안에 있는 prepare_gpt_dataset.py 스크립트를 통해 huggingface에 공개된 데이터셋을 다운로드하고, fine-tuning 학습에 바로 사용할 수 있도록 전처리를 진행하겠습니다.","전처리가 진행된 데이터셋은 gpt_dataset.pt 로 저장됩니다.","저장된 데이터셋은 코드상에서 다음과 같이 로드하여 사용할 수 있습니다."]}],[{"l":"2. Moreh의 학습 코드 톺아보기","p":["학습 데이터를 모두 준비하셨다면 다음으로는 실제 fine-tuning 과정을 실행할 train_gpt.py 스크립트의 내용을 살펴 보겠습니다. 이번 단계에서는 MoAI Platform은 pytorch와의 완전한 호환성으로 학습 코드가 일반적인 nvidia gpu를 위한 pytorch 코드와 100% 동일하다는 것을 확인하실 수 있습니다. 또한 이를 넘어서 기존의 복잡한 병렬화 기법들을 MoAI Platform에서는 얼마나 효율적으로 구현할 수 있는지도 확인하실 수 있습니다.","우선 제공된 스크립트를 그대로 사용하여 튜토리얼을 끝까지 진행해 보시기를 권장합니다. 이후 스크립트를 원하는 대로 수정하셔서 Cerebras-GPT-13B 모델, 혹은 다른 공개된 모델을 다른 방식으로 fine-tuning하는 것도 얼마든지 가능합니다. 필요하시다면 Moreh에서 제공하는 MoAI Platform 응용 가이드( LLM Fine-tuning 파라미터 가이드)를 참고하십시오."]},{"l":"Training Code","p":["모든 코드는 일반적인 pytorch 사용 경험과 완벽하게 동일합니다.","먼저, transformers 라이브러리에서 필요한 모듈을 불러옵니다.","HuggingFace에 공개된 모델 config와 체크포인트를 불러옵니다.","Fine tuning 준비하기 단계에서 저장한 전처리된 데이터셋을 불러와 데이터로더를 정의합니다.","이후 학습도 일반적인 Pytorch를 사용하여 모델 학습과 동일하게 진행됩니다.","위와 같이 MoAI Platform에서는 기존 pytorch 코드와 동일한 방식으로 작성하실 수 있습니다."]},{"l":"About Advanced Parallelism","p":["본 튜토리얼에 사용되는 학습 스크립트에서는 아래와 같은 코드가 추가로 한 줄 존재합니다. 이는 MoAI Platform에서 제공하는 최고의 병렬화 기능을 수행하는 코드입니다.","본 튜토리얼에서 사용하는 Cerebras-GPT-13B 과 같은 거대한 언어 모델의 경우 필연적으로 여러 개의 GPU를 사용하여 학습시켜야만 합니다. 이때, MoAI Platform이 아닌 다른 프레임워크를 사용할 경우, Data Parallel, Pipeline Parallel, Tensor Parallel과 같은 병렬화 기법을 도입하여 학습을 수행해야 합니다.","예를 들어, 사용자가 일반적인 pytorch 코드에서 DDP를 적용하고 싶다면, 다음과 같은 코드 스니펫이 추가되어야 합니다. ( https://pytorch.org/tutorials/intermediate/ddp_tutorial.html)","이와 같은 기본적인 세팅 이외에도 유저는 학습 스크립트 작성 과정에서 multi processing 환경에서의 Python 코드의 동작에 대해 이해하고 있어야 하며, 특히 multi node 세팅에서는 학습에 사용되는 노드들에 대한 환경 구성 작업이 추가적으로 들어가야 합니다. 게다가 모델의 종류, 크기, 데이터셋 등을 고려한 최적의 병렬화 방법을 찾기 위해서는 매우 많은 시간이 소요됩니다.","반면, MoAI Platform의 AP기능은 유저가 직접 이러한 추가적인 병렬화 기법을 적용할 필요 없이, 단지 학습 스크립트에 다음과 같은 코드 한 줄을 추가하는 것 만으로도 최적화된 병렬화 학습을 진행할 수 있습니다.","이렇게 MoAI Platform만의 Advanced Parallelization(AP)은 다른 프레임워크에서는 경험할 수 없는 병렬화의 최적화 및 자동화 기능입니다. 이를 통해 최적의 분산 병렬 처리 를 경험해 보실 수 있습니다. AP기능을 활용하면 대규모 모델 훈련 시 필요한 Pipeline Parallelism, Tensor Parallelism의 최적 매개변수와 환경변수 조합을 매우 간단한 코드 한 줄 로 설정할 수 있습니다."]}],[{"l":"3. 학습 실행하기","p":["이제 실제로 fine tuning을 실행해 보겠습니다."]},{"l":"가속기 Flavor 설정","p":["(모든 문서에 추가될 그림 생성 예정)","AMD MI210 GPU 32개 사용","AMD MI250 GPU 16개 사용","AMD MI300X GPU 8개 사용","KT Cloud의 Hyperscale AI Computing 사용 시: 선택","KT Hyperscale AI Computing (HAC) 서비스 가속기 모델 정보","LLM Fine-tuning 파라미터 가이드","MoAI Platform에서는 사용자에게 물리 GPU가 노출되지 않습니다. 대신 PyTorch에서 사용 가능한 가상의 MoAI Accelerator가 제공됩니다. 가속기의 flavor를 설정함으로써 실제로 PyTorch에서 물리 GPU를 얼마나 활용할지를 결정할 수 있습니다. 선택한 가속기 Flavor에 따라 총 학습 시간 및 GPU 사용 비용이 달라지므로 사용자의 학습 상황에 따른 판단이 필요합니다. 사용자의 학습 목표에 맞는 가속기 Flavor를 선택하기 위해 다음 문서를 참고하세요.","moreh-switch-model 툴을 사용하여 현재 시스템에서 사용 가능한 가속기 flavor 리스트를 확인할 수 있습니다. 원활한 모델 학습을 위해 moreh-switch-model 명령어를 이용해 더 큰 메모리의 MoAI Accelerator로 변경할 수 있습니다.","Moreh의 체험판 컨테이너 사용 시: 선택","q 를 입력해 변경을 완료합니다.","따라서 처음 설정되어 있던 flavor를 로 전환한 다음 moreh-smi 명령을 사용하여 정상적으로 반영되었는지 확인하겠습니다.","로 잘 변경된 것을 확인할 수 있습니다.","먼저 moreh-smi 명령어를 이용해 현재 사용중인 MoAI Accelerator를 확인합니다.","변경 사항이 잘 반영되었는지 확인하기 위해 다시 moreh-smi 명령어를 이용해 현재 사용중인 MoAI Accelerator를 확인합니다.","사용을 위해 8을 입력합니다.","앞서 GPT Fine-tuning 문서에서 MoAI Accelerator를 확인했던 것을 기억하시나요? 이제 본격적인 학습 실행을 위해 필요한 가속기를 설정해보겠습니다.","여기서 번호를 입력하여 다른 flavor로 전환할 수 있습니다.","이번 튜토리얼에서는 2048GB 크기의 MoAI Accelerator를 이용하겠습니다.","튜토리얼을 계속 진행하기 위해 인프라 제공자에게 각 flavor에 대응되는 GPU 종류 및 개수를 문의하십시오. 다음 중 하나에 해당하는 flavor를 선택하여 계속 진행하십시오.","현재 사용중인 MoAI Accelerator의 메모리 크기는 256GB입니다."]},{"l":"학습 실행","p":["주어진 train_gpt.py 스크립트를 실행합니다.","학습이 정상적으로 진행된다면 다음과 같은 로그가 출력 될 것입니다. 로그를 통해 최적의 병렬화 설정을 찾는 Advanced Parallelism 기능이 정상 동작하는 것을 확인할 수 있습니다. 앞서 살펴 본 PyTorch 스크립트 상에서는 AP 코드 한 줄을 제외한 다른 부분에서 GPU 여러 개를 동시에 사용하기 위한 처리가 전혀 없었음을 참고하십시오.","Loss 값이 다음과 같이 떨어지며 정상 학습이 이루어지는 것을 확인할 수 있습니다.","학습 도중에 출력되는 throughput은 해당 PyTorch 스크립트를 통해 초당 몇 개의 token을 학습하고 있는지를 의미합니다.","AMD MI250 GPU 16개 사용 시: 약 17000 tokens/sec","GPU 종류 및 개수에 따른 대략적인 학습 소요 시간은 다음과 같습니다.","AMD MI250 GPU 16개 사용 시: 약 00분"]},{"l":"학습 중에 가속기 상태 확인","p":["학습 도중에 터미널을 하나 더 띄워서 컨테이너에 접속한 후 moreh-smi 명령을 실행하시면 다음과 같이 MoAI Accelerator의 메모리를 점유하며 학습 스크립트가 실행되는 것을 확인하실 수 있습니다. 실행 로그상에서 초기화 과정이 끝나고 Step 1~ 15가 출력되는 도중에 확인해 보시기 바랍니다."]}],[{"l":"4. 학습 결과 확인하기","p":["앞 장과 같이 train_gpt.py 스크립트를 실행하면 결과 모델이 code_generation 디렉토리에 저장됩니다. 이는 순수한 PyTorch 모델 파라미터 파일로 MoAI Platform이 아닌 일반 GPU 서버에서도 100% 호환됩니다.","미리 다운로드한 GitHub 레포지토리의 tutorial 디렉토리 아래에 있는 inference_gpt.py 스크립트로 학습된 모델을 테스트해 볼 수 있습니다.","코드를 실행합니다.","출력값을 확인해보면 모델이 프롬프트 내용대로 적절한 함수를 생성한 것을 확인할 수 있습니다."]}],[{"l":"5. GPU 개수 변경하기","p":["앞과 동일한 fine-tuning 작업을 GPU 개수를 바꾸어 다시 실행해 보겠습니다. MoAI Platform은 GPU 자원을 단일 가속기로 추상화하여 제공하며 자동으로 병렬 처리를 수행합니다. 따라서 GPU 개수를 변경하더라도 PyTorch 스크립트는 전혀 고칠 필요가 없습니다."]},{"l":"가속기 Flavor 변경","p":["moreh-switch-model 툴을 사용하여 가속기 flavor를 전환합니다. 가속기 변경 방법은 3. 학습 실행하기 문서를 한번 더 참고해주시기 바랍니다.","인프라 제공자에게 문의하여 다음 중 하나를 선택한 다음 계속 진행하십시오. ( KT Hyperscale AI Computing (HAC) 서비스 가속기 모델 정보)","AMD MI250 GPU 32개 사용","Moreh의 체험판 컨테이너 사용 시: 선택","KT Cloud의 Hyperscale AI Computing 사용 시: 선택","AMD MI210 GPU 64개 사용","AMD MI300X GPU 16개 사용"]},{"l":"학습 실행","p":["다시 train_gpt.py 스크립트를 실행합니다.","사용 가능한 GPU 메모리가 2배가 늘었기 때문에, 배치 사이즈 또한 64로 변경하여 실행시켜 보겠습니다.","학습이 정상적으로 진행된다면 다음과 같은 로그가 출력될 것입니다.","앞서 GPU 개수가 절반이었을 때 실행한 결과와 비교해 동일하게 학습이 이루어지며 throughput이 향상되었음을 확인할 수 있습니다.","AMD MI250 GPU 16 → 32개 사용 시: 약 17000 tokens/sec → 34000 tokens/sec"]}],[{"l":"6. 마무리","p":["지금까지 MoAI Platform에서 HuggingFace에 공개된 GPT 기반 모델을 fine-tuning하는 과정을 살펴 보았습니다. MoAI Platform을 사용하면 기존의 학습 코드를 그대로 사용하면서 PyTorch 기반 오픈 소스 LLM 모델을 쉽게 GPU 클러스터에서 fine-tuning할 수 있습니다. 또한, MoAI 플랫폼을 사용한다면 여러분이 필요한 GPU 수를 코드 변경 없이 손쉽게 설정할 수 있습니다. 여러분만의 데이터로 새로운 모델을 빠르고 쉽게 개발해 보세요."]},{"l":"더 알아보기","p":["MoAI Platform의 자동병렬화 기능, Advanced Parallelization (AP)","Llama2 Fine-tuning","Mistral Fine-tuning","Baichuan2 Fine-tuning","Qwen Fine-tuning"]}],[{"l":"Qwen Fine-tuning","p":["이 튜토리얼은 MoAI Platform에서 오픈 소스 Qwen1.5 7B 모델을 fine-tuning 하는 예시를 소개합니다. 이를 통해 MoAI Platform으로 AMD GPU 클러스터를 사용하는 방법을 배우고 성능 및 자동 병렬화의 장점을 확인할 수 있습니다."]},{"l":"개요","p":["Qwen1.5 7B 모델은 중국의 Tongyi Qianwen(通义千问) 사에서 공개한 오픈소스 LLM입니다. 이 튜토리얼에서는 MoAI Platform에서 코드 생성(code generation) 태스크에 대해 시스템 프롬프트, 코드 생성을 위한 지시문, 입력값과 생성해야 할 코드로 구성되어 있는 python_code_instruction_18k_alpaca 데이터셋을 활용해 Qwen1.5 7B 모델을 fine-tuning 해보겠습니다."]},{"l":"시작하기 전에","p":["MoAI Platform 상의 컨테이너 혹은 가상 머신을 인프라 제공자로부터 발급받고, 여기에 SSH로 접속하는 방법을 안내 받으시기 바랍니다. 예를 들어 MoAI Platform 기반으로 운영되는 다음 퍼블릭 클라우드 서비스를 신청하여 사용할 수 있습니다.","KT Cloud의 Hyperscale AI Computing ( https://cloud.kt.com/solution/hyperscaleAiComputing/)","혹은 일시적으로 체험판 컨테이너 및 GPU 자원을 할당 받기를 원하시는 분은 Moreh에 문의하시기 바랍니다.","(Moreh 연락처 정보 추가 예정)","SSH로 접속한 다음 moreh-smi 명령을 실행하여 MoAI Accelerator가 잘 표시되는지 확인하시기 바랍니다. 디바이스 이름은 시스템마다 다르게 설정되어 있을 수 있습니다. 만약 이 과정에 문제가 있다면 인프라 제공자에게 문의하시거나 (troubleshooting 문서 추가 예정) 문서의 가이드를 참고하시기 바랍니다."]},{"l":"MoAI Accelerator 확인","p":["이 튜토리얼에서 안내할 Qwen 모델과 같은 sLLM을 학습하기 위해서는 적절한 크기의 MoAI Accelerator를 선택해야 합니다. 먼저 moreh-smi 명령어를 이용해 현재 사용중인 MoAI Accelerator를 확인합니다.","수행할 학습에 필요한 구체적인 MoAI Accelerator 설정에 대한 설명은 “3. 학습 실행하기”에서 제공하겠습니다."]}],[{"l":"1. Fine-tuning 준비하기","p":["MoAI Platform에서 PyTorch 스크립트 실행 환경을 준비하는 것은 일반적인 GPU 서버에서와 크게 다르지 않습니다."]},{"l":"PyTorch 설치 여부 확인하기","p":["SSH로 컨테이너에 접속한 다음 아래와 같이 실행하여 현재 conda 환경에 PyTorch가 설치되어 있는지 확인합니다.","버전명에는 PyTorch 버전과 이를 실행시키기 위한 MoAI 버전이 함께 표시되어 있습니다. 위 예시의 경우 PyTorch 1.13.1+cu116 버전을 실행하는 MoAI의 24.2.0 버전이 설치되어 있음을 의미합니다.","만약 conda: command not found 메시지가 표시되거나, torch 패키지가 리스트되지 않거나, 혹은 torch 패키지가 존재하더라도 버전명에 “moreh”가 포함되지 않은 경우 (Prepare Fine-tuning on MoAI Platform) 문서에 따라 conda 환경을 생성하십시오."]},{"l":"PyTorch 동작 여부 확인하기","p":["다음과 같이 실행하여 torch 패키지가 정상적으로 import되고 MoAI Accelerator가 인식되는지 확인합니다. 만약 이 과정에 문제가 생긴다면 (troubleshooting 문서 추가 예정) 문서에 따라 조치하십시오."]},{"l":"필요 Python 패키지 설치","p":["다음과 같이 실행하여 스크립트 실행에 필요한 서드 파티 Python 패키지들을 미리 설치합니다."]},{"l":"학습 스크립트 다운로드","p":["다음과 같이 실행하여 GitHub 레포지토리에서 학습을 위한 PyTorch 스크립트를 다운로드합니다. 본 튜토리얼에서는 tutorial 디렉토리 안에 있는 train_qwen.py 스크립트를 사용할 것입니다."]},{"l":"학습 데이터 다운로드","p":["학습 데이터를 다운로드하기 위해 dataset 디렉터리 안에 있는 prepare_qwen_dataset.py 스크립트를 사용하겠습니다. 코드를 실행하면 python_code_instruction_18k_alpaca 데이터를 다운로드하고 학습에 사용할 수 있도록 전처리를 진행하여 qwen_dataset.pt 파일로 저장합니다.","전처리가 진행된 데이터셋은 qwen_dataset.pt 로 저장됩니다.","저장된 데이터셋은 코드상에서 다음과 같이 로드하여 사용할 수 있습니다."]}],[{"l":"2. Moreh의 학습 코드 톺아보기","p":["학습 데이터를 모두 준비하셨다면 다음으로는 실제 fine-tuning 과정을 실행할 train_qwen.py 스크립트의 내용에 대해 살펴 보겠습니다. 이 스크립트는 통상적인 PyTorch 코드로서 Hugging Face Transformers 라이브러리에 있는 Qwen 모델 구현을 기반으로 fine-tuning 작업을 실행합니다.","이번 단계에서는 MoAI Platform은 pytorch와의 완전한 호환성으로 학습 코드가 일반적인 nvidia gpu를 위한 pytorch 코드와 100% 동일하다는 것을 확인하실 수 있습니다. 또한 이를 넘어서 기존의 복잡한 병렬화 기법들을 MoAI Platform에서는 얼마나 효율적으로 구현할 수 있는지도 확인하실 수 있습니다.","우선 제공된 스크립트를 그대로 사용하여 튜토리얼을 끝까지 진행해 보시기를 권장합니다. 이후 스크립트를 원하는 대로 수정하셔서 Qwen1.5 7B 모델을 다른 방식으로 fine-tuning하는 것도 얼마든지 가능합니다. MoAI Platform은 PyTorch와의 완전한 호환성을 제공하기 때문입니다. 필요하시다면 Moreh에서 제공하는 MoAI Platform 응용 가이드( LLM Fine-tuning 파라미터 가이드) 참고하십시오."]},{"l":"Training Code","p":["모든 코드는 일반적인 pytorch 사용 경험과 완벽하게 동일합니다.","먼저, transformers 라이브러리에서 필요한 모듈을 불러옵니다.","HuggingFace에 공개된 모델 config와 체크포인트를 불러옵니다.","Fine tuning 준비하기 단계에서 저장한 전처리된 데이터셋을 불러와 데이터로더를 정의합니다.","이후 학습도 일반적인 Pytorch를 사용하여 모델 학습과 동일하게 진행됩니다.","위와 같이 MoAI Platform에서는 기존 pytorch 코드와 동일한 방식으로 작성하실 수 있습니다."]},{"l":"About Advanced Parallelism","p":["본 튜토리얼에 사용되는 학습 스크립트에는 아래와 같은 코드가 추가로 한 줄 존재합니다. 이는 MoAI Platform에서 제공하는 자동 병렬화 기능을 수행하는 코드입니다.","본 튜토리얼에서 사용하는 Qwen1.5 7B 와 같은 거대한 언어 모델을 학습시키기 위해서는 필연적으로 여러 개의 GPU를 사용해야 합니다. 다른 프레임워크를 사용할 경우 Data Parallel, Pipeline Parallel, Tensor Parallel 등의 병렬화 기법을 도입하여 학습을 진행해야 합니다.","예를 들어, 사용자가 일반적인 pytorch 코드에서 DDP를 적용하고 싶다면, 다음과 같은 코드 스니펫이 추가되어야 합니다. ( https://pytorch.org/tutorials/intermediate/ddp_tutorial.html)","이러한 기본적인 설정 외에도, 사용자는 학습 스크립트 작성 과정에서 Python 코드가 multi processing 환경에서 어떻게 작동하는지 이해해야 합니다. 특히 multi node 세팅에서는 학습에 사용되는 각 노드의 환경 구성 작업이 추가로 필요합니다. 또한, 모델의 종류, 크기, 데이터셋 등을 고려하여 최적의 병렬화 방법을 찾는 작업은 상당한 시간이 소요됩니다.","반면 MoAI Platform을 사용하면 이러한 복잡한 병렬화 기법을 직접 구현할 필요 없이 학습 스크립트에 한 줄의 코드를 추가하는 것만으로도 최적화된 병렬화 학습을 진행할 수 있습니다.","이렇듯 다른 프레임워크에서는 경험할 수 없는 병렬화의 최적화 및 자동화 기능인 MoAI Platform만의 Advanced Parallelization(AP)을 통해 최적의 분산 병렬처리 를 경험해보시기 바랍니다. AP기능을 이용하면 일반적으로 대규모 모델 훈련시 필요한 Pipeline Parallelism, Tensor Parallelism의 최적 매개변수와 환경변수 조합을 아주 간단한 코드 한 줄 을 통해 확보할 수 있습니다."]}],[{"l":"3. 학습 실행하기","p":["이제 실제로 fine tuning을 실행해 보겠습니다."]},{"l":"가속기 Flavor 설정","p":["(모든 문서에 추가될 그림 생성 예정)","AMD MI210 GPU 32개 사용","AMD MI250 GPU 16개 사용","AMD MI300X GPU 8개 사용","KT Cloud의 Hyperscale AI Computing 사용 시: 선택","KT Hyperscale AI Computing (HAC) 서비스 가속기 모델 정보 문서를 참고하십시오.","LLM Fine-tuning 파라미터 가이드","MoAI Platform에서는 사용자에게 물리 GPU가 노출되지 않습니다. 대신 PyTorch에서 사용 가능한 가상의 MoAI Accelerator가 제공됩니다. 사용자는 가속기의 Flavor를 설정하여 실제 물리 GPU를 어떻게 활용할지 결정할 수 있습니다. 선택한 가속기 Flavor에 따라 총 학습 시간과 GPU 사용 비용이 달라지므로 사용자는 학습 상황에 따라 판단해야 합니다. 사용자의 학습 목표에 맞는 가속기 Flavor를 선택하기 위해 다음 문서를 참고하세요.","moreh-switch-model 툴을 사용하여 현재 시스템에서 사용 가능한 가속기 flavor 리스트를 확인할 수 있습니다. 원활한 모델 학습을 위해 moreh-switch-model 명령어를 이용해 더 큰 메모리의 MoAI Accelerator로 변경할 수 있습니다.","Moreh의 체험판 컨테이너 사용 시: 선택","q 를 입력해 변경을 완료합니다.","다음 중 하나에 해당하는 flavor를 선택하여 계속 진행하십시오.","따라서 처음 설정되어 있던 flavor를 로 전환한 다음 moreh-smi 명령을 사용하여 정상적으로 반영되었는지 확인하겠습니다.","로 잘 변경된 것을 확인할 수 있습니다.","먼저 moreh-smi 명령어를 이용해 현재 사용중인 MoAI Accelerator를 확인합니다.","변경 사항이 잘 반영되었는지 확인하기 위해 다시 moreh-smi 명령어를 이용해 현재 사용중인 MoAI Accelerator를 확인합니다.","사용을 위해 8을 입력합니다.","앞서 Qwen Fine-tuning 문서에서 MoAI Accelerator를 확인했던 것을 기억하시나요? 이제 본격적인 학습 실행을 위해 필요한 가속기를 설정해보겠습니다.","여기서 번호를 입력하여 다른 flavor로 전환할 수 있습니다.","위 문서를 참고하시거나 인프라 제공자에게 각 flavor에 대응되는 GPU 종류 및 개수를 문의하십시오.","이번 튜토리얼에서는 2048GB 크기의 MoAI Accelerator를 이용하겠습니다.","현재 사용중인 MoAI Accelerator의 메모리 크기는 512GB입니다."]},{"l":"학습 실행","p":["주어진 train_qwen.py 스크립트를 실행합니다.","학습이 정상적으로 진행된다면 다음과 같은 로그가 출력될 것입니다. 중간에 파란색으로 표시된 부분을 보시면 Advanced Parallelism 기능이 정상 동작하는 것을 확인할 수 있습니다. 앞서 살펴본 PyTorch 스크립트에서는 AP 코드 한 줄을 제외한 다른 부분에서 GPU 여러 개를 동시에 사용하기 위한 처리가 전혀 없었음을 참고하십시오.","Loss 값이 아래와 같이 나타나며 정상적으로 학습이 진행되고 있음을 확인할 수 있습니다.","학습 중에 표시되는 throughput은 해당 PyTorch 스크립트를 통해 초당 몇 개의 토큰을 학습하고 있는지를 의미합니다.","AMD MI250 GPU 16개 사용 시: 약 59,000 tokens/sec","GPU 종류 및 개수에 따른 대략적인 학습 소요 시간은 다음과 같습니다.","AMD MI250 GPU 16개 사용 시: 약 40분"]},{"l":"학습 중에 가속기 상태 확인","p":["학습 도중에 터미널을 하나 더 띄워서 컨테이너에 접속한 후 moreh-smi 명령을 실행하시면 다음과 같이 MoAI Accelerator의 메모리가 차지되는 것을 확인할 수 있습니다. 실행 로그를 보면 초기화 과정이 완료되고 Loss가 출력되는 도중에 확인해 보세요."]}],[{"l":"4. 학습 결과 확인하기","p":["앞 장과 같이 train_qwen.py 스크립트를 실행하면 결과 모델이 qwen_code_generation 디렉터리에 저장됩니다. 이는 순수한 PyTorch 모델 파라미터 파일로, MoAI Platform이 아닌 일반 GPU 서버에서도 완벽히 호환됩니다.","미리 다운로드한 GitHub 레포지토리의 tutorial 디렉터리 아래에 있는 inference_qwen.py 스크립트로 학습된 모델을 테스트해 볼 수 있습니다. 테스트에서는 \"주어진 문자열 리스트를 입력받아 공백으로 결합하는 함수를 만들어\"라는 프롬프트가 사용되었습니다.","코드를 실행합니다.","출력값을 확인해보면 모델이 프롬프트 내용대로 적절한 함수를 생성한 것을 확인할 수 있습니다."]}],[{"l":"5. GPU 개수 변경하기","p":["앞과 동일한 fine tuning 작업을 GPU 개수를 바꾸어 다시 실행해 보겠습니다. MoAI Platform은 GPU 자원을 단일 가속기로 추상화하여 제공하며 자동으로 병렬 처리를 수행합니다. 그러므로 GPU 개수를 변경하더라도 PyTorch 스크립트를 수정할 필요가 전혀 없습니다."]},{"l":"가속기 Flavor 변경","p":["moreh-switch-model 툴을 사용하여 가속기 flavor를 전환합니다. 가속기 변경 방법은 3. 학습 실행하기 문서를 한번 더 참고해주시기 바랍니다.","인프라 제공자에게 문의하여 다음 중 하나를 선택한 다음 계속 진행하십시오. ( KT Hyperscale AI Computing (HAC) 서비스 가속기 모델 정보)","AMD MI250 GPU 32개 사용","Moreh의 체험판 컨테이너 사용 시: 선택","KT Cloud의 Hyperscale AI Computing 사용 시: 선택","AMD MI210 GPU 64개 사용","AMD MI300X GPU 16개 사용"]},{"l":"학습 실행","p":["다시 별도의 배치 사이즈 변경 없이 train_qwen.py 스크립트를 실행합니다.","학습이 정상적으로 진행된다면 다음과 같은 로그가 출력될 것입니다.","앞서 GPU 개수가 절반이었을 때 실행한 결과와 비교해 동일하게 학습이 이루어지며 throughput이 향상되었음을 확인할 수 있습니다.","AMD MI250 GPU 16 → 32개 사용 시: 약 59,000 tokens/sec → 105,000 tokens/sec"]}],[{"l":"6. 마무리","p":["지금까지 MoAI Platform에서 Qwen1.5 7B 모델을 fine-tuning하는 과정을 살펴 보았습니다. MoAI Platform을 사용하면 기존의 학습 코드를 그대로 사용하면서 PyTorch 기반 오픈 소스 LLM 모델을 쉽게 GPU 클러스터에서 fine-tuning할 수 있습니다. 또한, MoAI 플랫폼을 사용한다면 여러분이 필요한 GPU 수를 코드 변경 없이 손쉽게 설정할 수 있습니다. 여러분만의 데이터로 새로운 모델을 빠르고 쉽게 개발해 보세요."]},{"l":"더 알아보기","p":["MoAI Platform의 자동병렬화 기능, Advanced Parallelization (AP)","Llama2 Fine-tuning","Mistral Fine-tuning","GPT Fine-tuning","Baichuan2 Fine-tuning"]}],[{"l":"Baichuan2 Fine-tuning","p":["이 튜토리얼은 MoAI Platform에서 오픈 소스 Baichuan2 13B 모델을 fine-tuning하는 예시를 소개합니다. 튜토리얼을 통해 MoAI Platform으로 AMD GPU 클러스터를 사용하는 방법을 익히고 성능 및 자동 병렬화의 이점을 확인할 수 있습니다."]},{"l":"개요","p":["Baichuan2는 Baichuan Intelligent Technology 가 개발한 오픈 소스, 대규모 다국어 언어 모델입니다. 이 모델은 2조 6천억 개의 토큰으로 구성된 방대한 데이터 세트에서 훈련된 70억 및 130억 매개 변수로 사용할 수 있는 구성을 가지고 있습니다.","이 튜토리얼에서는 MoAI Platform에서 text-generation e-commerce 데이터셋인 Bitext-customer-support-llm-chatbot-training-dataset 을 활용해 Baichuan2 13B 모델을 fine-tuning 해보겠습니다."]},{"l":"시작하기 전에","p":["MoAI Platform 상의 컨테이너 혹은 가상 머신을 인프라 제공자로부터 발급받고, 여기에 SSH로 접속하는 방법을 안내 받으시기 바랍니다. 예를 들어 MoAI Platform 기반으로 운영되는 다음 퍼블릭 클라우드 서비스를 신청하여 사용할 수 있습니다.","KT Cloud의 Hyperscale AI Computing ( https://cloud.kt.com/solution/hyperscaleAiComputing/)","혹은 일시적으로 체험판 컨테이너 및 GPU 자원을 할당 받기를 원하시는 분은 Moreh에 문의하시기 바랍니다.","(Moreh 연락처 정보 추가 예정)","SSH로 접속한 다음 moreh-smi 명령을 실행하여 MoAI Accelerator가 잘 표시되는지 확인하시기 바랍니다. 디바이스 이름은 시스템마다 다르게 설정되어 있을 수 있습니다. 만약 이 과정에 문제가 있다면 인프라 제공자에게 문의하시거나 (troubleshooting 문서 추가 예정) 문서의 가이드를 참고하시기 바랍니다."]},{"l":"MoAI Accelerator 확인","p":["이 튜토리얼에서 안내할 Mistral 7B 모델과 같은 sLLM을 학습하기 위해서는 적절한 크기의 MoAI Accelerator를 선택해야 합니다. 먼저 moreh-smi 명령어를 이용해 현재 사용중인 MoAI Accelerator를 확인합니다.","수행할 학습에 필요한 구체적인 MoAI Accelerator 설정에 대한 설명은 ‘3. 학습 실행하기’에서 제공하겠습니다."]}],[{"l":"1. Fine-tuning 준비하기","p":["MoAI Platform에서 PyTorch 스크립트 실행 환경을 준비하는 것은 일반적인 GPU 서버에서와 크게 다르지 않습니다."]},{"l":"PyTorch 설치 여부 확인하기","p":["SSH로 컨테이너에 접속한 다음 아래와 같이 실행하여 현재 conda 환경에 PyTorch가 설치되어 있는지 확인합니다.","버전명에는 PyTorch 버전과 이를 실행시키기 위한 MoAI 버전이 함께 표시되어 있습니다. 위 예시의 경우 PyTorch 1.13.1+cu116 버전을 실행하는 MoAI의 24.3.0 버전이 설치되어 있음을 의미합니다.","만약 conda: command not found 메시지가 표시되거나, torch 패키지가 리스트되지 않거나, 혹은 torch 패키지가 존재하더라도 버전명에 “moreh”가 포함되지 않은 경우 (Prepare Fine-tuning on MoAI Platform) 문서에 따라 conda 환경을 생성하십시오."]},{"l":"PyTorch 동작 여부 확인하기","p":["다음과 같이 실행하여 torch 패키지가 정상적으로 import되고 MoAI Accelerator가 인식되는지 확인합니다. 만약 이 과정에 문제가 생긴다면 (troubleshooting 문서 추가 예정) 문서에 따라 조치하십시오."]},{"l":"필요 Python 패키지 설치","p":["다음과 같이 실행하여 스크립트 실행에 필요한 서드 파티 Python 패키지들을 미리 설치합니다."]},{"l":"학습 스크립트 다운로드","p":["다음과 같이 실행하여 GitHub 레포지토리에서 학습을 위한 PyTorch 스크립트를 다운로드합니다. 본 튜토리얼에서는 tutorial 디렉토리 안에 있는 train_baichuan2_13b.py 스크립트를 사용할 것입니다."]},{"l":"학습 데이터 다운로드","p":["이번 튜토리얼에서 사용할 학습 데이터를 다운로드 받기 위해 dataset 디렉토리 안에 있는 prepare_baichuan_dataset.py 스크립트를 사용하겠습니다. 코드를 실행하면 e-commerce 데이터인 Bitext-custormer-support-llm-chatbot 데이터를 다운로드 받고 정제한 후 baichuan_dataset.pt 파일로 저장합니다.","전처리가 진행된 데이터셋은 baichuan_dataset.pt 로 저장됩니다.","저장된 데이터셋은 코드상에서 다음과 같이 로드하여 사용할 수 있습니다."]}],[{"l":"2. Moreh의 학습 코드 톺아보기","p":["학습 데이터를 모두 준비하셨다면 다음으로는 실제 fine-tuning 과정을 실행할 train_baichuan2_13b.py 스크립트의 내용을 살펴 보겠습니다. 이번 단계에서는 MoAI Platform은 pytorch와의 완전한 호환성으로 학습 코드가 일반적인 nvidia gpu를 위한 pytorch 코드와 100% 동일하다는 것을 확인하실 수 있습니다. 또한 이를 넘어서 기존의 복잡한 병렬화 기법들을 MoAI Platform에서는 얼마나 효율적으로 구현할 수 있는지도 확인하실 수 있습니다.","우선 제공된 스크립트를 그대로 사용하여 튜토리얼을 끝까지 진행해 보시기를 권장합니다. 이후 스크립트를 원하는 대로 수정하셔서 Baichuan 모델을 다른 방식으로 fine-tuning하는 것도 얼마든지 가능합니다. 필요하시다면 Moreh에서 제공하는 MoAI Platform 응용 가이드( LLM Fine-tuning 파라미터 가이드)를 참고하십시오."]},{"l":"Training Code","p":["모든 코드는 일반적인 pytorch 사용 경험과 완벽하게 동일합니다.","먼저, transformers 라이브러리에서 필요한 모듈을 불러옵니다.","HuggingFace에 공개된 모델 config와 체크포인트를 불러옵니다.","Fine tuning 준비하기 단계에서 저장한 전처리된 데이터셋을 불러와 데이터로더를 정의합니다.","이후 학습도 일반적인 Pytorch를 사용하여 모델 학습과 동일하게 진행됩니다.","위와 같이 MoAI Platform에서는 기존에 사용하시던 PyTorch 스크립트를 수정 없이 동일하게 사용하실 수 있습니다."]},{"l":"About Advanced Parallelism","p":["본 튜토리얼에 사용되는 학습 스크립트에는 아래와 같은 코드가 추가로 한 줄 존재합니다. 이는 MoAI Platform에서 제공하는 자동 병렬화 기능을 수행하는 코드입니다.","본 튜토리얼에서 사용하는 Baichuan2 13B 와 같은 거대한 언어 모델의 경우 필연적으로 여러 개의 GPU를 사용하여 학습시켜야만 합니다. 이때, MoAI Platform이 아닌 다른 프레임워크를 사용할 경우, Data Parallel, Pipeline Parallel, Tensor Parallel과 같은 병렬화 기법을 도입하여 학습을 수행해야 합니다.","예를 들어, 사용자가 일반적인 pytorch 코드에서 DDP를 적용하고 싶다면, 다음과 같은 코드 스니펫이 추가되어야 합니다. ( https://pytorch.org/tutorials/intermediate/ddp_tutorial.html)","이와 같은 기본적인 세팅 이외에도 유저는 학습 스크립트 작성 과정에서 multi processing 환경에서의 Python 코드의 동작에 대해 이해하고 있어야 하며, 특히 multi node 세팅에서는 학습에 사용되는 노드들에 대한 환경 구성 작업이 추가적으로 들어가야 합니다. 게다가 모델의 종류, 크기, 데이터셋 등을 고려한 최적의 병렬화 방법을 찾기 위해서는 매우 많은 시간이 소요됩니다.","반면, MoAI Platform의 AP기능은 유저가 직접 이러한 추가적인 병렬화 기법을 적용할 필요 없이, 단지 학습 스크립트에 다음과 같은 코드 한 줄을 추가하는 것 만으로도 최적화된 병렬화 학습을 진행할 수 있습니다.","이렇듯 다른 프레임워크에서는 경험할 수 없는 병렬화의 최적화 및 자동화 기능인 MoAI Platform만의 Advanced Parallelization(AP)을 통해 최적의 분산 병렬처리 를 경험해보시기 바랍니다. AP기능을 이용하면 일반적으로 대규모 모델 훈련시 필요한 Pipeline Parallelism, Tensor Parallelism의 최적 매개변수와 환경변수 조합을 아주 간단한 코드 한 줄 을 통해 확보할 수 있습니다."]}],[{"l":"3. 학습 실행하기","p":["이제 실제로 fine tuning을 실행해 보겠습니다."]},{"l":"가속기 Flavor 설정","p":["(모든 문서에 추가될 그림 생성 예정)","4xLarge.2048GB 로 잘 변경된 것을 확인할 수 있습니다.","AMD MI210 GPU 32개 사용","AMD MI250 GPU 16개 사용","AMD MI300X GPU 8개 사용","KT Cloud의 Hyperscale AI Computing 사용 시: 선택","KT Hyperscale AI Computing (HAC) 서비스 가속기 모델 정보 문서를 참고하십시오.","LLM Fine-tuning 파라미터 가이드","MoAI Platform에서는 사용자에게 물리 GPU가 노출되지 않습니다. 대신 PyTorch에서 사용 가능한 가상의 MoAI Accelerator가 제공됩니다. 가속기의 flavor를 설정함으로써 실제로 PyTorch에서 물리 GPU를 얼마나 활용할지를 결정할 수 있습니다. 선택한 가속기 Flavor에 따라 총 학습 시간과 GPU 사용 비용이 달라지므로 사용자의 학습 상황에 따른 판단이 필요합니다. 사용자의 학습 목표에 맞는 가속기 Flavor를 선택하기 위해 다음 문서를 참고하세요.","moreh-switch-model 툴을 사용하여 현재 시스템에서 사용 가능한 가속기 flavor 리스트를 확인할 수 있습니다. 원활한 모델 학습을 위해 moreh-switch-model 명령어를 이용해 더 큰 메모리의 MoAI Accelerator로 변경할 수 있습니다.","Moreh의 체험판 컨테이너 사용 시: 선택","q 를 입력해 변경을 완료합니다.","따라서 처음 설정되어 있던 flavor를 로 전환한 다음 moreh-smi 명령을 사용하여 정상적으로 반영되었는지 확인하겠습니다.","먼저 moreh-smi 명령어를 이용해 현재 사용중인 MoAI Accelerator를 확인합니다.","변경 사항이 잘 반영되었는지 확인하기 위해 다시 moreh-smi 명령어를 이용해 현재 사용중인 MoAI Accelerator를 확인합니다.","사용을 위해 8을 입력합니다.","앞서 ‘ Baichuan2 Finetuning’ 문서에서 MoAI Accelerator를 확인했던 것을 기억하시나요? 이제 본격적인 학습 실행을 위해 필요한 가속기를 설정해보겠습니다.","여기서 번호를 입력하여 다른 flavor로 전환할 수 있습니다.","이번 튜토리얼에서는 2048GB 크기의 MoAI Accelerator를 이용하겠습니다.","튜토리얼을 계속 진행하기 위해 인프라 제공자에게 각 flavor에 대응되는 GPU 종류 및 개수를 문의하십시오. 다음 중 하나에 해당하는 flavor를 선택하여 계속 진행하십시오.","현재 사용중인 MoAI Accelerator의 메모리 크기는 256GB입니다."]},{"l":"학습 실행","p":["주어진 train_baichuan2_13b.py 스크립트를 실행합니다.","학습이 정상적으로 진행된다면 다음과 같은 로그가 출력 될 것입니다. 로그를 통해 최적의 병렬화 설정을 찾는 Advanced Parallelism 기능이 정상 동작하는 것을 확인할 수 있습니다. 앞서 살펴 본 PyTorch 스크립트 상에서는 AP 코드 한 줄을 제외한 다른 부분에서 GPU 여러 개를 동시에 사용하기 위한 처리가 전혀 없었음을 참고하십시오.","Loss 값이 다음과 같이 떨어지며 정상 학습이 이루어지는 것을 확인할 수 있습니다.","학습 도중에 출력되는 throughput은 해당 PyTorch 스크립트를 통해 초당 몇 개의 token을 학습하고 있는지를 의미합니다.","AMD MI250 GPU 8개 사용 시: 약 191605 tokens/sec","GPU 종류 및 개수에 따른 대략적인 학습 소요 시간은 다음과 같습니다.","AMD MI250 GPU 8개 사용 시: 약 30분"]},{"l":"학습 중에 가속기 상태 확인","p":["학습 도중에 터미널을 하나 더 띄워서 컨테이너에 접속한 후 moreh-smi 명령을 실행하시면 다음과 같이 MoAI Accelerator의 메모리를 점유하며 학습 스크립트가 실행되는 것을 확인하실 수 있습니다. 실행 로그상에서 초기화 과정이 끝나고 Loss가 출력되는 도중에 확인해 보시기 바랍니다."]}],[{"l":"4. 학습 결과 확인하기","p":["앞 장과 같이 train_baichuan2_13b.py 스크립트를 실행하면 결과 모델이 baichuan_code_generation 디렉토리에 저장됩니다. 이는 순수한 PyTorch 모델 파라미터 파일로 MoAI Platform이 아닌 일반 GPU 서버에서도 100% 호환됩니다.","미리 다운로드한 GitHub 레포지토리의 tutorial 디렉토리 아래에 있는 inference_baichuan.py 스크립트로 학습된 모델을 테스트해 볼 수 있습니다.","코드를 실행합니다.","출력값을 확인해보면 모델이 프롬프트에 대한 적절한 답변 생성한 것을 확인할 수 있습니다."]}],[{"l":"5. GPU 개수 변경하기","p":["앞과 동일한 fine-tuning 작업을 GPU 개수를 바꾸어 다시 실행해 보겠습니다. MoAI Platform은 GPU 자원을 단일 가속기로 추상화하여 제공하며 자동으로 병렬 처리를 수행합니다. 따라서 GPU 개수를 변경하더라도 PyTorch 스크립트는 전혀 고칠 필요가 없습니다."]},{"l":"가속기 Flavor 변경","p":["moreh-switch-model 툴을 사용하여 가속기 flavor를 전환합니다. 가속기 변경 방법은 3. 학습 실행하기 문서를 한번 더 참고해주시기 바랍니다.","인프라 제공자에게 문의하여 다음 중 하나를 선택한 다음 계속 진행하십시오. ( KT Hyperscale AI Computing (HAC) 서비스 가속기 모델 정보)","AMD MI250 GPU 32개 사용","Moreh의 체험판 컨테이너 사용 시: 선택","KT Cloud의 Hyperscale AI Computing 사용 시: 선택","AMD MI210 GPU 64개 사용","AMD MI300X GPU 16개 사용"]},{"l":"학습 실행","p":["다시 train_baichuan2_13b.py 스크립트를 실행합니다.","사용 가능한 GPU 메모리가 2배가 늘었기 때문에, 배치 사이즈 또한 2048로 변경하여 실행시켜 보겠습니다.","학습이 정상적으로 진행된다면 다음과 같은 로그가 출력될 것입니다.","앞서 GPU 개수가 절반이었을 때 실행한 결과와 비교해 동일하게 학습이 이루어지며 throughput이 향상되었음을 확인할 수 있습니다.","AMD MI250 GPU 16 → 32개 사용 시: 약 198,000 tokens/sec → 370,000 tokens/sec"]}],[{"l":"6. 마무리","p":["지금까지 MoAI Platform에서 HuggingFace에 공개된 Baichuan2 13B 모델을 fine-tuning하는 과정을 살펴 보았습니다. MoAI Platform을 사용하면 기존의 학습 코드를 그대로 사용하면서 PyTorch 기반 오픈 소스 LLM 모델을 쉽게 GPU 클러스터에서 fine-tuning할 수 있습니다. 또한, MoAI 플랫폼을 사용한다면 여러분이 필요한 GPU 수를 코드 변경 없이 손쉽게 설정할 수 있습니다. 여러분만의 데이터로 새로운 모델을 빠르고 쉽게 개발해 보세요."]},{"l":"더 알아보기","p":["MoAI Platform의 자동병렬화 기능, Advanced Parallelization (AP)","Llama2 Fine-tuning","Mistral Fine-tuning","GPT Fine-tuning","Qwen Fine-tuning"]}],[{"l":"Supported Documents"}],[{"i":"advanced-parallelization-ap","l":"Advanced Parallelization (AP)"},{"i":"moai-platform의-자동병렬화-기능--advanced-parallelization-ap","l":"MoAI Platform의 자동병렬화 기능, Advanced Parallelization (AP)"},{"i":"병렬화가-반드시-필요한-이유는-무엇일까요","l":"병렬화가 반드시 필요한 이유는 무엇일까요?","p":["일반적으로 많이 사용되는 Llama2 13B 모델의 크기를 Bytes 단위로 계산해봅시다.","130억개의 파라미터를 가진 Llama2 13B는 FP16 데이터 형을 기준으로 13B * 2 bytes 크기입니다. 이는 약 24.2GB입니다. AdamW optimizer는 13B * 2 * 2로 약 48.4GB입니다. 모델을 로드하는 데에만 최소 72.6GB가 필요하며 Gradient에 필요한 메모리 약 24.2GB 등 모델 로드 이외에 +\\alpha 의 메모리가 필요합니다.","MoAI Platform은 가상화 된 GPU 하나를 제공하지만, 이는 실제로 여러 GPU에 모델을 복사하고 batch samples를 균등하게 분할하여 학습시키는 DDP 방식으로 기본 설정되어 동작합니다. 따라서 1 device chip의 VRAM인 64GB가 넘는 데이터를 로드할 수 없습니다.","따라서 모델을 병렬화하여 여러 GPU에 로드하는 기법이 필요합니다."]},{"i":"advanced-parallelization이란","l":"Advanced Parallelization이란?","p":["Advanced Parallelization(이하 AP)은 MoAI Platform에서 제공하는 최적화된 분산 병렬처리 기능입니다. 일반적으로 ML 엔지니어라면 모델 병렬화를 ‘최적화’하기 위해 수 많은 경험적 시행착오를 겪곤합니다. (예를 들어, 모델의 stages 개수, micro batches 개수 등) 하지만 MoAI Platform을 사용한다면 다른 프레임워크에서는 경험할 수 없는 특별한 AP 기능을 활용할 수 있어 최적화된 병렬화에 소요되는 시간과 노력을 획기적으로 줄일 수 있습니다.","Moreh의 AP 기능은 기존 최적화 과정을 자동화 함으로써, 최적의 병렬화 환경 변수 조합을 신속하게 결정합니다. 따라서 대규모 모델 훈련시 적용하는 효율적인 Pipeline Parallelism, Tensor Parallelism의 최적 매개변수와 환경 변수 조합을 간단히 얻을 수 있습니다."]}],[{"i":"advanced-parallelism-ap-기능-사용하기","l":"Advanced Parallelism (AP) 기능 사용하기","p":["기본적으로 AP는 노드 단위로 병렬화를 진행합니다. 따라서 AP를 사용하기 위해서는 multi gpu 환경이어야 합니다. 아래 가이드를 따라 AP 기능을 사용하기에 앞서 사용자가 현재 사용하는 가속기 정보를 한번 더 점검해주시기 바랍니다. 가속기 사이즈에 대한 세부 정보는 KT Hyperscale AI Computing (HAC) 서비스 가속기 모델 정보 참고해주시기 바랍니다."]},{"l":"AP 기능 적용 방법","p":["AP 기능은 두가지 방식으로 적용할 수 있습니다.","코드 한줄 추가하기","실행 코드에 다음 한줄을 추가하여 AP 기능을 킬 수 있습니다. (이를 주석처리하면 끌 수 있습니다.)","환경 변수로 입력하기","다음과 같이 터미널 세션의 환경변수로 AP 기능을 킬 수 있습니다. ( 0으로 설정하면 끌 수 있습니다.)"]},{"l":"사용 예시 살펴보기","p":["-","1024","13,015,864,320","4xlarge","64","batch size","num params","Pytorch 환경 설정이 되었다면, Github 레포지토리에서 학습을 위한 코드를 가져옵니다.","quickstart 레포지토리를 클론하여 quickstart/ap-example 디렉토리를 확인해보시면 Moreh에서 미리 준비한 AP기능 test를 위한 text_summarization_for_ap.py 를 확인하실 수 있습니다. 이 코드를 기반으로 AP 기능을 적용해봅시다.","sda","sequence length","text_summarization_for_ap.py(전체코드 제공)","먼저 AP를 적용시키는 부분이 어디인지 python 프로그램에서 확인해보시죠.","사용자가 2대 이상의 노드를 사용하는 환경이 준비 되었다면 이제 AP 기능을 사용하기 위한 학습 코드를 만들어 보겠습니다. 이 가이드에서는 Llama2 모델을 활용하여 코드를 세팅합니다. 참고로, Llama2 모델은 커뮤니티 라이센스 동의와 Hugging Face 토큰 정보가 필요합니다. 1. Fine-tuning 준비하기 를 참고하여 학습 코드를 준비해주세요.","테스트를 위한 학습 구성은 다음과 같습니다. 이를 토대로 테스트를 진행하겠습니다.","학습 코드가 준비되었다면, MoAI Platform에서 학습을 실행하기 전 아래와 같이 pytorch 환경을 설정합니다. 아래 예시의 경우 PyTorch 1.13.1+cu116 버전을 실행하는 MoAI의 24.2.0 버전이 설치되어 있음을 의미합니다. 자세한 설명은 1. Fine-tuning 준비하기 튜토리얼을 참고해주시기 바랍니다."]},{"l":"AP 기능 ON","p":["프로그램의 main 함수 시작 지점에 AP 기능을 켜는 line이 있습니다. 다음과 같이 AP를 적용한 후 학습을 실행합니다.","학습이 종료되면 다음과 같은 로그를 확인할 수 있습니다.","이처럼 단 한 줄의 AP 기능 프로그램을 추가하여 복잡한 분산 병렬처리가 수행되어 학습이 진행된 것을 확인할 수 있습니다. AP 기능을 적용하여 손쉬운 병렬화가 가능했는데요, 만약 사용자가 AP 기능을 사용하지 않았을 때는 어떤 경험을 하게 될까요?"]},{"l":"AP 기능 OFF","p":["이를 확인할 수 있도록 AP를 켜지 않았을 때의 형상을 보여 드리겠습니다. 다시 python 프로그램의 main 함수 시작 지점에 AP 기능을 켜는 line을 주석처리하여 AP 기능을 끄겠습니다.","그 다음 학습을 진행합니다.","학습이 종료되면 다음과 같은 로그를 확인할 수 있습니다.","위 로그에서 RuntimeError: Error Code 4: OUT_OF_MEMORY 라는 메시지를 볼 수 있는데, 이것이 바로 앞서 말씀드린 1 device chip의 VRAM인 64GB가 넘는 데이터를 로드 할 수 없기 때문에 발생하는 OOM 에러입니다.","MoAI Platform 이 아닌 다른 프레임워크를 사용한다면 이런 불편함을 겪어야 합니다. 그러나 MoAI Platform을 사용하는 사용자라면 별도 병렬화 최적화를 위해 오랫동안 계산하며 고민하는 시간을 들이지 않고 AP 기능 한줄을 적용하여 골치아픈 OOM 문제를 해결할 수 있습니다. 정말 편리한 기능이죠?"]}],[{"l":"Prepare Fine-tuning on MoAI Platform","p":["MoAI Platform은 다양한 GPU로 구성될 수 있지만, 동일한 인터페이스(CLI)를 통해 사용자에게 일관된 경험을 제공합니다. 모든 사용자가 같은 방식으로 시스템에 접근하여 플랫폼을 사용할 수 있기 때문에 보다 효율적이며 직관적입니다.","MoAI Platform 또한 일반적인 AI 학습 환경과 유사하게 Python 기반의 프로그래밍을 지원합니다. 이에 따라 본 문서에서는 AI 학습을 위한 표준 환경 구성으로서 conda 가상 환경의 설정과 사용 방법을 중심으로 설명합니다."]},{"l":"conda 환경 설정하기","p":["훈련을 시작하기 위해 먼저 conda 환경을 생성합니다.","my-env 에는 사용자가 사용할 환경 이름을 입력합니다.","conda 환경을 활성화합니다.","Fine-tuning에 필요한 library와 package를 설치합니다.","moreh-smi 명령어를 입력해 설치된 Moreh 솔루션의 버전과 사용중인 MoAI Accelerator 정보를 확인할 수 있습니다. 현재 사용중인 MoAI Accelerator는 4xLarge.2048GB 입니다. MoAI Accelerator에 대한 자세한 정보는 MoAI Accelerator 사양을 참고해주세요."]},{"i":"moai-accelerator-선택-변경하기","l":"MoAI Accelerator 선택, 변경하기","p":["sLLM 파인튜닝시 학습 데이터 배치 사이즈에 따른 적절한 MoAI Accelerator 모델을 moreh toolkit을 사용하여 선택, 변경할 수 있습니다. 참고로, sLLM(약 7B~ 13B 모델)을 fine-tuning 하기 위해 일반적으로 사용되는 데이터셋의 크기는 약 40GB의 텍스트 데이터셋입니다.","먼저, moreh-smi 를 사용하여 현재 사용하고 있는 MoAI Accelerator 모델을 확인해 보겠습니다.","현재 사용하고 있는 MoAI Accelerator 모델에서 제공되는 메모리는 64GB입니다. moreh-switch-model 을 사용하여 더 큰 메모리를 제공하는 MoAI Accelerator 모델로 변경해 보겠습니다.","4xLarge.2048GB 모델로 변경하기 위해 8 을 입력합니다.","q 를 입력하여 변경을 완료합니다.","다시 moreh-smi 를 사용하여 변경된 상태를 확인하면 사용하고 있는 MoAI Accelerator 모델이 4xLarge.2048GB 모델로 변경된 것을 확인할 수 있습니다.","각 모델별로 MoAI Platform에서 권장하는 Fine-tuning 시 최적의 파라미터는 LLM Fine-tuning 파라미터 가이드 를 참고하시기 바랍니다.","moreh-smi, moreh-switch-model 를 비롯한 moreh toolkit의 구체적인 사용 방법에 대해서는 MoAI Platform의 toolkit 사용하기 를 참고하시기 바랍니다."]}],[{"i":"kt-hyperscale-ai-computing-hac-서비스-가속기-모델-정보","l":"KT Hyperscale AI Computing (HAC) 서비스 가속기 모델 정보","p":["…","12xLarge.6144GB","12대","1대","24xLarge.12288GB","24대","2xLarge.1024GB","2대","3xLarge.1536GB","3대","48xLarge.24576GB","48대","4xLarge.2048GB","4대","6xLarge.3072GB","6대","8xLarge.4096GB","8대","https://manual.cloud.kt.com/kt/hyperscale-ai-computing-howtouse-cj","KT Cloud 공식 매뉴얼 바로가기","Large.256GB","Medium.128GB","MI250 0.5개","MI250 12개","MI250 16개","MI250 192개","MI250 1개","MI250 24개","MI250 2개","MI250 32개","MI250 48개","MI250 4개","MI250 8개","MI250 96개","Model","Small.64GB","xLarge.512GB","노드 수","실제 물리 GPU","현재 HAC 서비스는 AMD MI250 GPU를 사용해 구동되고 있습니다. 모델/애플리케이션에 따라 성능이 달라질 수 있지만 기본적으로 AMD MI250 하나와 NVIDIA A100 하나에서 동등한 성능이 나온다고 예상하시면 됩니다."]}],[{"l":"LLM Fine-tuning parameter guide","p":["1,122,745 MiB","1,138,546 MiB","1,403,047 MiB","1,651,008 MiB","1,680,233 MiB","1,706,797 MiB","1,764,955 MiB","1,767,888 MiB","1,800,656 MiB","1024","109872","11,562","11m","121,013","125,180","128","1292886 MiB","12m","13286","1360m","13m","1403,2189","144,124","1467646 MiB","1489,3","15,890","15,972","154,12123","157,859","16","1600235 MiB","163,839","172395","17m","186,353","191605","194,282","2,146,115 MiB","2,645,347 MiB","2,800,000 MiB","2,845,656 MiB","2048","20m","22m","238,212","24,156","24.2.0","24.3.0","24.5.0","24m","256","25m","26,111","27B","28m","2xlarge","3,460,240 MiB","3013826 MiB","30m","3143781 MiB","3181616 MiB","32","32,371","32,563","34m","35m","36m","376,493","38m","400m","4096","40m","442,982 MiB","47,679","480m","4xlarge","50,782","51,353","512","543816 MiB","56,385","560,835 MiB","560m","58531","586m","59m","62,481","62,582","62,740","626,391 MiB","62m","63,893","638,460 MiB","64","65,565","6841","69,840","720m","749065 MiB","784,485 MiB","790,572 MiB","790454 MiB","7m","8,934","81m","843,375 MiB","858,128 MiB","866,656 MiB","872m","8m","8xlarge","92,623","93,165","962m","99,873","9m","Advanced Parallelism 적용 유무","Baichuan2 13B","batch size","Cerebras GPT 13B","Llama2 13B","Mistral 7B","MoAI Accelerator","MoAI Accelerator 에 명시된 명칭은 사용자가 이용하는 CSP에 따라 다를 수 있습니다.","MoAI Platform version","Qwen1.5 7B","sequence length","throughput","token 갯수","True","vram 사용량","모델명","이 가이드는 MoAI Platform에서 제공하는 최적의 파라미터이며 사용자 학습시 참고 정보로만 사용해주시기 바랍니다.","학습 시간"]}]]